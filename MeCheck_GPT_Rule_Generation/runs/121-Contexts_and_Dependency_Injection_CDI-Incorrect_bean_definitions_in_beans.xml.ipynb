{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87bdfd",
   "metadata": {
    "id": "NAkKL_FFFr4w",
    "papermill": {
     "duration": 0.0,
     "end_time": "2025-11-13T03:40:50.806490",
     "exception": false,
     "start_time": "2025-11-13T03:40:50.806490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ead7b7c",
   "metadata": {
    "id": "3Kf1GK8LGWL0",
    "papermill": {
     "duration": 0.0,
     "end_time": "2025-11-13T03:40:50.822411",
     "exception": false,
     "start_time": "2025-11-13T03:40:50.822411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From now on, please replace your ChatGPT usage with your GPT API key (GPT-5 model). In this way, you don’t have to copy and paste things back-and-forth to prompt GPT. Instead, you can import input from a CSV or export output to a CSV file.\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/3715772\n",
    "\n",
    "Please also ensure the temperature setting of the GPT model you use to be 0, to minimize randomness.For each jetbrain rule, please provide the syntax/examples/built-in functions. Ask GPT to output (1) the corresponding RSL rule based on the chosen library/framework, (2) explanation of the newly introduced functions if there is a new function, (3) URL of the third-party data source supporting that rule, (4) summary of the description from the third-party web page supporting that rule, and (5) its (model's) validation of the generated rule based on the jetbrain rule as well as the located third-party data source.\n",
    "The prompt mimics Chain-of-Thought as it asks GPT to provide supporting facts, and validate the generated rule based on the given fact as well as retrieved support fact.\n",
    "\n",
    "Please explore to memorize the syntax/examples/built-in functions by calling certain GPT function(s). If this is not possible, please include the syntax and built-in functions in each prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bc59d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:40:50.822411Z",
     "iopub.status.busy": "2025-11-13T03:40:50.822411Z",
     "iopub.status.idle": "2025-11-13T03:40:50.841913Z",
     "shell.execute_reply": "2025-11-13T03:40:50.841913Z"
    },
    "id": "t1HSsrAhG3NT",
    "outputId": "853fd7a2-fced-470f-d77d-233f1bab4e94",
    "papermill": {
     "duration": 0.019502,
     "end_time": "2025-11-13T03:40:50.841913",
     "exception": false,
     "start_time": "2025-11-13T03:40:50.822411",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Global constants - - further automation by feeding a csv file input containing the info blow\n",
    "FRAMEWORK = \"Spring MVC\"\n",
    "TOPIC = \"Mismatch in @PathVariable declarations and usages\"\n",
    "\n",
    "SOURCE = \"https://www.jetbrains.com.cn/en-us/help/inspectopedia/MVCPathVariableInspection.html\"\n",
    "\n",
    "\n",
    "\n",
    "OUT_DIR = \"./runs\"\n",
    "ARTIFACTS_DIR = \"./artifacts/Mismatch_in_PathVariable_declarations_and_usages\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7640d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:40:50.854107Z",
     "iopub.status.busy": "2025-11-13T03:40:50.854107Z",
     "iopub.status.idle": "2025-11-13T03:40:50.858237Z",
     "shell.execute_reply": "2025-11-13T03:40:50.858237Z"
    },
    "papermill": {
     "duration": 0.016324,
     "end_time": "2025-11-13T03:40:50.858237",
     "exception": false,
     "start_time": "2025-11-13T03:40:50.841913",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "FRAMEWORK = \"Contexts and Dependency Injection (CDI)\"\n",
    "SOURCE = \"https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html\"\n",
    "TOPIC = \"Incorrect bean definitions in beans.xml\\ufeff\"\n",
    "OUT_DIR = \"C:\\\\Users\\\\spenc\\\\Downloads\\\\local\\\\runs\"\n",
    "ARTIFACTS_DIR = \"C:\\\\Users\\\\spenc\\\\Downloads\\\\local\\\\artifacts\\\\Incorrect_bean_definitions_in_beans.xml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e459961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:40:50.869878Z",
     "iopub.status.busy": "2025-11-13T03:40:50.869878Z",
     "iopub.status.idle": "2025-11-13T03:40:53.325200Z",
     "shell.execute_reply": "2025-11-13T03:40:53.325200Z"
    },
    "id": "GcjiYs-ho_88",
    "papermill": {
     "duration": 2.466963,
     "end_time": "2025-11-13T03:40:53.325200",
     "exception": false,
     "start_time": "2025-11-13T03:40:50.858237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no openaikey\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import userdata\n",
    "# import time, tempfile, os, re, json, requests\n",
    "# from typing import List, Set, Dict, Optional\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import pandas as pd\n",
    "# from openai import OpenAI\n",
    "\n",
    "\n",
    "import os, re, json, time, tempfile, requests\n",
    "from typing import List, Set, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "NUM_RULES = 1\n",
    "CODEX5_SUGGESTING_NUM_RULES = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\", \"\")\n",
    "# OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_API_KEY = None\n",
    "\n",
    "\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"no openaikey\")\n",
    "if not SERPAPI_API_KEY:\n",
    "    print(\"no serpapikey\")\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def serpapi_google(q: str, num: int = 10) -> List[Dict]:\n",
    "    if not SERPAPI_API_KEY:\n",
    "        return []\n",
    "    try:\n",
    "        params = {\"engine\": \"google\", \"q\": q, \"api_key\": SERPAPI_API_KEY, \"num\": str(num)}\n",
    "        r = requests.get(\"https://serpapi.com/search\", params=params, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"organic_results\", [])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def locate_inspectopedia_url(framework: str, topic: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Use SerpAPI to find the Inspectopedia page for the given framework/topic.\n",
    "    Returns the first JetBrains.cn inspectopedia URL it finds.\n",
    "    \"\"\"\n",
    "    queries = [\n",
    "        f'site:jetbrains.com.cn Inspectopedia {FRAMEWORK} {TOPIC}',\n",
    "        f'site:jetbrains.com.cn help Inspectopedia {FRAMEWORK} {TOPIC}',\n",
    "    ]\n",
    "    print(queries)\n",
    "    for q in queries:\n",
    "        for item in serpapi_google(q, num=10):\n",
    "            url = item.get(\"link\", \"\")\n",
    "            if \"inspectopedia\" in url.lower() and \"jetbrains.com.cn\" in url.lower():\n",
    "                print(url)\n",
    "                return url\n",
    "    return None\n",
    "\n",
    "def isSOURCE(url: str) -> bool:\n",
    "    if locate_inspectopedia_url(FRAMEWORK, TOPIC) == SOURCE:\n",
    "        return url\n",
    "    else:\n",
    "        return SOURCE\n",
    "\n",
    "def get_jetbrains_description(url: str) -> str:\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(f\"--user-data-dir={tempfile.mkdtemp()}\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    span = soup.find(\"span\", class_=\"sub-title sub-title--related-h1\")\n",
    "    if not span:\n",
    "        return \"(no span marker found)\"\n",
    "\n",
    "    text_parts = []\n",
    "    for sib in span.find_all_next():\n",
    "        if sib.name in (\"h2\", \"section\"):\n",
    "            break\n",
    "        if sib.name == \"p\":\n",
    "            text_parts.append(sib.get_text(\" \", strip=True))\n",
    "    return \" \".join(text_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46e3700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:40:53.336150Z",
     "iopub.status.busy": "2025-11-13T03:40:53.336150Z",
     "iopub.status.idle": "2025-11-13T03:41:05.419629Z",
     "shell.execute_reply": "2025-11-13T03:41:05.419629Z"
    },
    "id": "Gs-d3gm_Xmsc",
    "papermill": {
     "duration": 12.088545,
     "end_time": "2025-11-13T03:41:05.419629",
     "exception": false,
     "start_time": "2025-11-13T03:40:53.331084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the JetBrains inspection description\n",
    "CONTENT = get_jetbrains_description(SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c70ae0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:05.447234Z",
     "iopub.status.busy": "2025-11-13T03:41:05.447234Z",
     "iopub.status.idle": "2025-11-13T03:41:05.465080Z",
     "shell.execute_reply": "2025-11-13T03:41:05.465080Z"
    },
    "id": "lt2vsCgdXR0t",
    "outputId": "c0343247-af9f-4d7e-f4d0-c76d3a1ee6d8",
    "papermill": {
     "duration": 0.027818,
     "end_time": "2025-11-13T03:41:05.465080",
     "exception": false,
     "start_time": "2025-11-13T03:41:05.437262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reports incorrect bean definitions in beans.xml configuration files.\n"
     ]
    }
   ],
   "source": [
    "print(CONTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fabe9bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:05.478815Z",
     "iopub.status.busy": "2025-11-13T03:41:05.478815Z",
     "iopub.status.idle": "2025-11-13T03:41:05.506621Z",
     "shell.execute_reply": "2025-11-13T03:41:05.506621Z"
    },
    "id": "rmt96UWopGpw",
    "outputId": "71ed1eb5-0d0c-4184-b66c-a78a6bf8ad45",
    "papermill": {
     "duration": 0.041541,
     "end_time": "2025-11-13T03:41:05.506621",
     "exception": false,
     "start_time": "2025-11-13T03:41:05.465080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>purpose</th>\n",
       "      <th>signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>callExists</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a specific call exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classExists</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a class exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getArg</td>\n",
       "      <td>code</td>\n",
       "      <td>Get an argument from a call/method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>getClasses</td>\n",
       "      <td>code</td>\n",
       "      <td>Get a collection of classes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getConstructors</td>\n",
       "      <td>code</td>\n",
       "      <td>Get constructors of a class</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>getFamily</td>\n",
       "      <td>code</td>\n",
       "      <td>Get type hierarchy/family information</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>getFields</td>\n",
       "      <td>code</td>\n",
       "      <td>Get fields of a class</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>getFQN</td>\n",
       "      <td>code</td>\n",
       "      <td>Get fully qualified name</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>getMethods</td>\n",
       "      <td>code</td>\n",
       "      <td>Get methods of a class</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>getName</td>\n",
       "      <td>code</td>\n",
       "      <td>Get the simple name/identifier</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>getReturnType</td>\n",
       "      <td>code</td>\n",
       "      <td>Get a method's return type</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>getSN</td>\n",
       "      <td>code</td>\n",
       "      <td>Get simple name (SN)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>getType</td>\n",
       "      <td>code</td>\n",
       "      <td>Get type information</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hasField</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a class has a specific field</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hasParam</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a method has a specific parameter</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hasParamType</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a method has a parameter of a gi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>indexInBound</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether an index is within bounds</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>isIterable</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether a type is iterable</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>isLibraryClass</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether the class comes from libraries</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>isUniqueSN</td>\n",
       "      <td>code</td>\n",
       "      <td>Check whether the simple name is unique</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>locateClassSN</td>\n",
       "      <td>code</td>\n",
       "      <td>Locate a class by simple name</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>locateClassFQN</td>\n",
       "      <td>code</td>\n",
       "      <td>Locate a class by fully qualified name</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>getAnnoAttr</td>\n",
       "      <td>annotation</td>\n",
       "      <td>Get an annotation attribute value</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>getAnnoAttrNames</td>\n",
       "      <td>annotation</td>\n",
       "      <td>Get the set of annotation attribute names</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>getAnnotated</td>\n",
       "      <td>annotation</td>\n",
       "      <td>Get elements annotated with a given annotation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hasAnnotation</td>\n",
       "      <td>annotation</td>\n",
       "      <td>Check whether an element has a given annotation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hasAnnoAttr</td>\n",
       "      <td>annotation</td>\n",
       "      <td>Check whether an annotation attribute exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>elementExists</td>\n",
       "      <td>xml</td>\n",
       "      <td>Check whether an XML element exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>getAttr</td>\n",
       "      <td>xml</td>\n",
       "      <td>Get an XML attribute value</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>getAttrs</td>\n",
       "      <td>xml</td>\n",
       "      <td>Get a set of XML attributes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>getElms</td>\n",
       "      <td>xml</td>\n",
       "      <td>Get a set of XML elements</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>getXMLs</td>\n",
       "      <td>xml</td>\n",
       "      <td>Get XML documents/fragments</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>hasAttr</td>\n",
       "      <td>xml</td>\n",
       "      <td>Check whether an XML attribute exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>endsWith</td>\n",
       "      <td>misc</td>\n",
       "      <td>Check whether a string ends with a suffix</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>isEmpty</td>\n",
       "      <td>misc</td>\n",
       "      <td>Check whether a string/collection is empty</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>indexOf</td>\n",
       "      <td>misc</td>\n",
       "      <td>Return the index of a substring/element</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>join</td>\n",
       "      <td>misc</td>\n",
       "      <td>Join/concatenate values</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>pathExists</td>\n",
       "      <td>misc</td>\n",
       "      <td>Check whether a filesystem/project path exists</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>substring</td>\n",
       "      <td>misc</td>\n",
       "      <td>Extract a substring</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>startsWith</td>\n",
       "      <td>misc</td>\n",
       "      <td>Check whether a string starts with a prefix</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>upperCase</td>\n",
       "      <td>misc</td>\n",
       "      <td>Convert a string to upper case</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name    category  \\\n",
       "0         callExists        code   \n",
       "1        classExists        code   \n",
       "2             getArg        code   \n",
       "3         getClasses        code   \n",
       "4    getConstructors        code   \n",
       "5          getFamily        code   \n",
       "6          getFields        code   \n",
       "7             getFQN        code   \n",
       "8         getMethods        code   \n",
       "9            getName        code   \n",
       "10     getReturnType        code   \n",
       "11             getSN        code   \n",
       "12           getType        code   \n",
       "13          hasField        code   \n",
       "14          hasParam        code   \n",
       "15      hasParamType        code   \n",
       "16      indexInBound        code   \n",
       "17        isIterable        code   \n",
       "18    isLibraryClass        code   \n",
       "19        isUniqueSN        code   \n",
       "20     locateClassSN        code   \n",
       "21    locateClassFQN        code   \n",
       "22       getAnnoAttr  annotation   \n",
       "23  getAnnoAttrNames  annotation   \n",
       "24      getAnnotated  annotation   \n",
       "25     hasAnnotation  annotation   \n",
       "26       hasAnnoAttr  annotation   \n",
       "27     elementExists         xml   \n",
       "28           getAttr         xml   \n",
       "29          getAttrs         xml   \n",
       "30           getElms         xml   \n",
       "31           getXMLs         xml   \n",
       "32           hasAttr         xml   \n",
       "33          endsWith        misc   \n",
       "34           isEmpty        misc   \n",
       "35           indexOf        misc   \n",
       "36              join        misc   \n",
       "37        pathExists        misc   \n",
       "38         substring        misc   \n",
       "39        startsWith        misc   \n",
       "40         upperCase        misc   \n",
       "\n",
       "                                              purpose signature  \n",
       "0                Check whether a specific call exists            \n",
       "1                        Check whether a class exists            \n",
       "2                  Get an argument from a call/method            \n",
       "3                         Get a collection of classes            \n",
       "4                         Get constructors of a class            \n",
       "5               Get type hierarchy/family information            \n",
       "6                               Get fields of a class            \n",
       "7                            Get fully qualified name            \n",
       "8                              Get methods of a class            \n",
       "9                      Get the simple name/identifier            \n",
       "10                         Get a method's return type            \n",
       "11                               Get simple name (SN)            \n",
       "12                               Get type information            \n",
       "13         Check whether a class has a specific field            \n",
       "14    Check whether a method has a specific parameter            \n",
       "15  Check whether a method has a parameter of a gi...            \n",
       "16            Check whether an index is within bounds            \n",
       "17                   Check whether a type is iterable            \n",
       "18       Check whether the class comes from libraries            \n",
       "19            Check whether the simple name is unique            \n",
       "20                      Locate a class by simple name            \n",
       "21             Locate a class by fully qualified name            \n",
       "22                  Get an annotation attribute value            \n",
       "23          Get the set of annotation attribute names            \n",
       "24     Get elements annotated with a given annotation            \n",
       "25    Check whether an element has a given annotation            \n",
       "26       Check whether an annotation attribute exists            \n",
       "27                Check whether an XML element exists            \n",
       "28                         Get an XML attribute value            \n",
       "29                        Get a set of XML attributes            \n",
       "30                          Get a set of XML elements            \n",
       "31                        Get XML documents/fragments            \n",
       "32              Check whether an XML attribute exists            \n",
       "33          Check whether a string ends with a suffix            \n",
       "34         Check whether a string/collection is empty            \n",
       "35            Return the index of a substring/element            \n",
       "36                            Join/concatenate values            \n",
       "37     Check whether a filesystem/project path exists            \n",
       "38                                Extract a substring            \n",
       "39        Check whether a string starts with a prefix            \n",
       "40                     Convert a string to upper case            "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Newly generated rules expressed in RSL would use only these built-in functions for now to minizme the work of modification of the current engine, MeCheck.\n",
    "\n",
    "builtins_data = [\n",
    "    # Code-related\n",
    "    {\"name\":\"callExists\",\"category\":\"code\",\"purpose\":\"Check whether a specific call exists\",\"signature\":\"\"},\n",
    "    {\"name\":\"classExists\",\"category\":\"code\",\"purpose\":\"Check whether a class exists\",\"signature\":\"\"},\n",
    "    {\"name\":\"getArg\",\"category\":\"code\",\"purpose\":\"Get an argument from a call/method\",\"signature\":\"\"},\n",
    "    {\"name\":\"getClasses\",\"category\":\"code\",\"purpose\":\"Get a collection of classes\",\"signature\":\"\"},\n",
    "    {\"name\":\"getConstructors\",\"category\":\"code\",\"purpose\":\"Get constructors of a class\",\"signature\":\"\"},\n",
    "    {\"name\":\"getFamily\",\"category\":\"code\",\"purpose\":\"Get type hierarchy/family information\",\"signature\":\"\"},\n",
    "    {\"name\":\"getFields\",\"category\":\"code\",\"purpose\":\"Get fields of a class\",\"signature\":\"\"},\n",
    "    {\"name\":\"getFQN\",\"category\":\"code\",\"purpose\":\"Get fully qualified name\",\"signature\":\"\"},\n",
    "    {\"name\":\"getMethods\",\"category\":\"code\",\"purpose\":\"Get methods of a class\",\"signature\":\"\"},\n",
    "    {\"name\":\"getName\",\"category\":\"code\",\"purpose\":\"Get the simple name/identifier\",\"signature\":\"\"},\n",
    "    {\"name\":\"getReturnType\",\"category\":\"code\",\"purpose\":\"Get a method's return type\",\"signature\":\"\"},\n",
    "    {\"name\":\"getSN\",\"category\":\"code\",\"purpose\":\"Get simple name (SN)\",\"signature\":\"\"},\n",
    "    {\"name\":\"getType\",\"category\":\"code\",\"purpose\":\"Get type information\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasField\",\"category\":\"code\",\"purpose\":\"Check whether a class has a specific field\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasParam\",\"category\":\"code\",\"purpose\":\"Check whether a method has a specific parameter\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasParamType\",\"category\":\"code\",\"purpose\":\"Check whether a method has a parameter of a given type\",\"signature\":\"\"},\n",
    "    {\"name\":\"indexInBound\",\"category\":\"code\",\"purpose\":\"Check whether an index is within bounds\",\"signature\":\"\"},\n",
    "    {\"name\":\"isIterable\",\"category\":\"code\",\"purpose\":\"Check whether a type is iterable\",\"signature\":\"\"},\n",
    "    {\"name\":\"isLibraryClass\",\"category\":\"code\",\"purpose\":\"Check whether the class comes from libraries\",\"signature\":\"\"},\n",
    "    {\"name\":\"isUniqueSN\",\"category\":\"code\",\"purpose\":\"Check whether the simple name is unique\",\"signature\":\"\"},\n",
    "    {\"name\":\"locateClassSN\",\"category\":\"code\",\"purpose\":\"Locate a class by simple name\",\"signature\":\"\"},\n",
    "    {\"name\":\"locateClassFQN\",\"category\":\"code\",\"purpose\":\"Locate a class by fully qualified name\",\"signature\":\"\"},\n",
    "\n",
    "    # Annotation-related\n",
    "    {\"name\":\"getAnnoAttr\",\"category\":\"annotation\",\"purpose\":\"Get an annotation attribute value\",\"signature\":\"\"},\n",
    "    {\"name\":\"getAnnoAttrNames\",\"category\":\"annotation\",\"purpose\":\"Get the set of annotation attribute names\",\"signature\":\"\"},\n",
    "    {\"name\":\"getAnnotated\",\"category\":\"annotation\",\"purpose\":\"Get elements annotated with a given annotation\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasAnnotation\",\"category\":\"annotation\",\"purpose\":\"Check whether an element has a given annotation\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasAnnoAttr\",\"category\":\"annotation\",\"purpose\":\"Check whether an annotation attribute exists\",\"signature\":\"\"},\n",
    "\n",
    "    # XML-related\n",
    "    {\"name\":\"elementExists\",\"category\":\"xml\",\"purpose\":\"Check whether an XML element exists\",\"signature\":\"\"},\n",
    "    {\"name\":\"getAttr\",\"category\":\"xml\",\"purpose\":\"Get an XML attribute value\",\"signature\":\"\"},\n",
    "    {\"name\":\"getAttrs\",\"category\":\"xml\",\"purpose\":\"Get a set of XML attributes\",\"signature\":\"\"},\n",
    "    {\"name\":\"getElms\",\"category\":\"xml\",\"purpose\":\"Get a set of XML elements\",\"signature\":\"\"},\n",
    "    {\"name\":\"getXMLs\",\"category\":\"xml\",\"purpose\":\"Get XML documents/fragments\",\"signature\":\"\"},\n",
    "    {\"name\":\"hasAttr\",\"category\":\"xml\",\"purpose\":\"Check whether an XML attribute exists\",\"signature\":\"\"},\n",
    "\n",
    "    # Miscellaneous\n",
    "    {\"name\":\"endsWith\",\"category\":\"misc\",\"purpose\":\"Check whether a string ends with a suffix\",\"signature\":\"\"},\n",
    "    {\"name\":\"isEmpty\",\"category\":\"misc\",\"purpose\":\"Check whether a string/collection is empty\",\"signature\":\"\"},\n",
    "    {\"name\":\"indexOf\",\"category\":\"misc\",\"purpose\":\"Return the index of a substring/element\",\"signature\":\"\"},\n",
    "    {\"name\":\"join\",\"category\":\"misc\",\"purpose\":\"Join/concatenate values\",\"signature\":\"\"},\n",
    "    {\"name\":\"pathExists\",\"category\":\"misc\",\"purpose\":\"Check whether a filesystem/project path exists\",\"signature\":\"\"},\n",
    "    {\"name\":\"substring\",\"category\":\"misc\",\"purpose\":\"Extract a substring\",\"signature\":\"\"},\n",
    "    {\"name\":\"startsWith\",\"category\":\"misc\",\"purpose\":\"Check whether a string starts with a prefix\",\"signature\":\"\"},\n",
    "    {\"name\":\"upperCase\",\"category\":\"misc\",\"purpose\":\"Convert a string to upper case\",\"signature\":\"\"},\n",
    "]\n",
    "\n",
    "builtins_df = pd.DataFrame(builtins_data)\n",
    "builtins_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c53186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:05.510397Z",
     "iopub.status.busy": "2025-11-13T03:41:05.510397Z",
     "iopub.status.idle": "2025-11-13T03:41:05.522996Z",
     "shell.execute_reply": "2025-11-13T03:41:05.522996Z"
    },
    "id": "34nShSNJyNbE",
    "papermill": {
     "duration": 0.012599,
     "end_time": "2025-11-13T03:41:05.522996",
     "exception": false,
     "start_time": "2025-11-13T03:41:05.510397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Text Registry (single source of truth) ===\n",
    "RSL_SYNTAX = r\"\"\"Specification := Rule Id Body\n",
    "Body := '{' Stmt Stmt* '}'\n",
    "Stmt := ForStmt | IfStmt | AssertStmt | DeclStmt ';'\n",
    "\n",
    "ForStmt := 'for' '(' Type Id 'in' Exp ')' Body\n",
    "IfStmt := 'if' '(' Exp ')' Body\n",
    "\n",
    "AssertStmt := 'assert' '(' Exp ')' '{' MsgStmt ';' '}'\n",
    "MsgStmt := 'msg' '(' ',' SimExp (',' SimExp)* ')'\n",
    "\n",
    "DeclStmt := Type Id '=' Exp\n",
    "\n",
    "Exp := SimExp\n",
    "     | SimExp AND Exp\n",
    "     | SimExp OR  Exp\n",
    "     | NOT Exp\n",
    "\n",
    "SimExp := Id\n",
    "        | Lit\n",
    "        | FunctionCall\n",
    "        | '(' Exp ')'\n",
    "        | FunctionCall '==' SimExp\n",
    "        | exists '(' Type Id in Exp ')' '(' Exp ')'\n",
    "\n",
    "Type := '⟨' Id '⟩' | file | class | method | field | String\n",
    "Lit := StringLit | CharLit | IntLit | FloatLit\n",
    "FunctionCall := Id '(' Params ')'\n",
    "Params := SimExp (',' SimExp)*\"\"\"\n",
    "\n",
    "# Sample Rules are Rule #1, #3, and #5. If these change into some other subset, the output of newly generated rule may differ.\n",
    "\n",
    "RSL_EXAMPLE_RULES = r\"\"\"// Rule 1 — bean-class-exists\n",
    "Rule bean-class-exists {\n",
    "  for (file xml in getXMLs()) {\n",
    "    if (elementExists(xml, \"<bean>\")) {\n",
    "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
    "        String beanClassFQN = getAttr(bean, \"class\");\n",
    "        if (NOT isEmpty(beanClassFQN)) {\n",
    "          assert ( classExists(beanClassFQN) OR isLibraryClass(beanClassFQN) ) {\n",
    "            msg(\"Bean class: %s mentioned in bean: %s, does not exist\",\n",
    "                beanClassFQN, getName(bean));\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// Rule 3 — constructor-arg-name-field-map\n",
    "Rule constructor-arg-name-field-map {\n",
    "  for (file xml in getXMLs()) {\n",
    "    if (elementExists(xml, \"<bean>\")) {\n",
    "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
    "        String beanClassFQN = getAttr(bean, \"class\");\n",
    "        if (classExists(beanClassFQN)) {\n",
    "          class c = locateClassFQN(beanClassFQN);\n",
    "          for (<constructor-arg> constructor_arg in getElms(bean, \"<constructor-arg>\")) {\n",
    "            String arg_name = getAttr(constructor_arg, \"name\");\n",
    "            if (NOT isEmpty(arg_name)) {\n",
    "              assert ( exists(method con in getConstructors(c)) ( hasParam(con, arg_name) ) ) {\n",
    "                msg(\"The name of <constructor-arg>: %s in bean: %s does not correspond to any constructor parameter in class: %s\",\n",
    "                    arg_name, getName(bean), getFQN(c));\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// Rule 5 — constructor-index-out-of-bound\n",
    "Rule constructor-index-out-of-bound {\n",
    "  for (file xml in getXMLs()) {\n",
    "    if (elementExists(xml, \"<bean>\")) {\n",
    "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
    "        String beanClassFQN = getAttr(bean, \"class\");\n",
    "        if (classExists(beanClassFQN)) {\n",
    "          class c = locateClassFQN(beanClassFQN);\n",
    "          for (<constructor-arg> constructor_arg in getElms(bean, \"<constructor-arg>\")) {\n",
    "            if (hasAttr(constructor_arg, \"index\")) {\n",
    "              String arg_idx = getAttr(constructor_arg, \"index\");\n",
    "              assert ( exists(method constructor in getConstructors(c)) ( indexInBound(constructor, arg_idx) ) ) {\n",
    "                msg(\"Constructor index: %s of bean for class: %s in xml: %s is out of bound\",\n",
    "                    arg_idx, getFQN(c), getName(xml));\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb01d3ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:05.526654Z",
     "iopub.status.busy": "2025-11-13T03:41:05.526654Z",
     "iopub.status.idle": "2025-11-13T03:41:08.147316Z",
     "shell.execute_reply": "2025-11-13T03:41:08.147316Z"
    },
    "id": "xqG-6kW3inob",
    "outputId": "f65bfa22-8f02-4598-90ee-ba59b6a44b05",
    "papermill": {
     "duration": 2.620662,
     "end_time": "2025-11-13T03:41:08.147316",
     "exception": false,
     "start_time": "2025-11-13T03:41:05.526654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['\"CDI\"', '\"beans.xml\"', '\"IncorrectBeanDefinitions\"']\n",
      "Scores: [15, 12, 10]\n"
     ]
    }
   ],
   "source": [
    "keyword_message = {\"role\": \"user\",\n",
    "                   \"content\": f'''\n",
    "                    Context:\n",
    "                    Notice the inspection from JetBrains's Inspectopedia relating to\n",
    "                    the {FRAMEWORK} framework or library -\n",
    "                    {TOPIC}.\n",
    "                    Additionally, here is the source for the topic:\n",
    "                    {SOURCE}\n",
    "\n",
    "                    Goal:\n",
    "                    Grab 3 keywords from this topic that would be helpful for\n",
    "                    searching third party sources relating to this specific inspection.\n",
    "                    Have the keywords related soley on the framework and topic mentioned\n",
    "                    rather than the JetBrains or Inspectopedia.\n",
    "                    Return ONLY the keywords found with no explanation or additional\n",
    "                    content and return each keyword as a single word or at least without\n",
    "                    whitespace. Additionally, give each keyword a priority score in terms\n",
    "                    of which keyword would give the best search results. Each score will\n",
    "                    be a value between 1-15. Return the keywords and their respective\n",
    "                    scores in the following format:\n",
    "                    \"keyword_a keyword_b keyword_c\\nscore_a score_b score_c\"\n",
    "                    '''}\n",
    "\n",
    "def get_topic_keywords() -> tuple[list[str], list[int]]:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        print(\"⚠️ OPENAI_API_KEY is missing in environment (.env).\")\n",
    "        return [], []\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        messages=[keyword_message]\n",
    "    )\n",
    "\n",
    "    response_text = (response.choices[0].message.content or \"\").strip()\n",
    "\n",
    "\n",
    "    lines = response_text.split(\"\\n\")\n",
    "    if len(lines) == 2:\n",
    "        keywords = lines[0].split()\n",
    "        try:\n",
    "            scores = [int(x) for x in lines[1].split()]\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Score line is not integers.\")\n",
    "            return keywords, []\n",
    "        return keywords, scores\n",
    "\n",
    "    print(\"⚠️ Unexpected response format from OpenAI API.\")\n",
    "    return [], []\n",
    "\n",
    "KEYWORDS, SCORES = get_topic_keywords()\n",
    "print(\"Keywords:\", KEYWORDS)\n",
    "print(\"Scores:\", SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b013f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:08.162979Z",
     "iopub.status.busy": "2025-11-13T03:41:08.162979Z",
     "iopub.status.idle": "2025-11-13T03:41:08.169088Z",
     "shell.execute_reply": "2025-11-13T03:41:08.169088Z"
    },
    "id": "fwS_1QVd5yLp",
    "papermill": {
     "duration": 0.021772,
     "end_time": "2025-11-13T03:41:08.169088",
     "exception": false,
     "start_time": "2025-11-13T03:41:08.147316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_ORIGINAL_TMPL = r\"\"\"You are an expert in metadata used in the {{FRAMEWORK}} library/framework.\n",
    "Here is the core syntax of the language you will translate in plain English natural language per rule. This is the core syntax of Rule-Specific Language (RSL): {{RSL_SYNTAX}}\n",
    "\n",
    "Here is the currently built-in functions: {{BUILTIN_REFERENCE}}\n",
    "Here are three example rules expressed in RSL: {{EXAMPLE_RULES}}\n",
    "\n",
    "As sample rules, I shared three rules with you.\n",
    "Your job is to create one rule that corresponds to the topic {{TOPIC}} with the following content: {{CONTENT}}.\n",
    "\n",
    "The JetBrains Inspectopedia source page for this rule is: {{SOURCE}}.\n",
    "For the field \"source URL (JetBrains web page)\", output exactly {{SOURCE}}.\n",
    "\n",
    "Like the sample rules, it should be described following the syntax of RSL. You briefly describe what the new rule checks. The new rule must use only the built-in functions. This means that you should not introduce new built-in functions while creating a new rule corresponding to the topic and content. In the \"new_built-in_functions_explanation if any\" field, provide names of all built-in functions used in the new rule after confirming that no new functions were introduced. Also, there should not be any comments or BOM (byte order mark) to save the rule in a txt file.\n",
    "\n",
    "Then, locate the most relevant, at most, two 3rd-party web pages that address the the topic, content, and the generated rule in terms of metadata-related bugs. Provide a brief summary per the 3rd-party web page content in one field together. Finally, provide a post-model validation based on generated rule and the located most relevant 3rd-party web page(s).\n",
    "Prioritize finding 3rd-party web pages with at least one of the following keywords in the URLs to ensure relevancy: [{{KEYWORDS}}]. The keywords found in the URLs do not have to be exact matches to the keywords in the list.\n",
    "\n",
    "Return the output strictly as a valid JSON array, not text, not markdown, not explanation.\n",
    "You must NOT include any text, commentary, or code fences (like ```).\n",
    "You must NOT prepend or append any text before or after the JSON array.\n",
    "\n",
    "Each JSON object must include exactly these fields:\n",
    "[\n",
    "  \"framework\",\n",
    "  \"source URL (JetBrains web page)\",\n",
    "  \"brief_description of the content in the source URL\",\n",
    "  \"GPT-4 generated_rule\",\n",
    "  \"generated_rule_explanation from GPT-4\",\n",
    "  \"new_built-in_functions_explanation if any\",\n",
    "  \"PRE_model_validation\",\n",
    "  \"3rd-party most relevant URLs and summary for each URL\",\n",
    "  \"POST_model_validation\"\n",
    "]\n",
    "\n",
    "For the \"brief_description of the content in the source URL\" field, provide a concise summary of the content provided in the prompt ({{CONTENT}}), which comes from the JetBrains page at {{SOURCE}}. Do NOT describe any other URL.\n",
    "\n",
    "The output must start with \"[\" and end with \"]\".\n",
    "If you cannot find information for any field, use an empty string (\"\").\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e5c8dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:08.178956Z",
     "iopub.status.busy": "2025-11-13T03:41:08.178956Z",
     "iopub.status.idle": "2025-11-13T03:41:08.184941Z",
     "shell.execute_reply": "2025-11-13T03:41:08.184941Z"
    },
    "id": "zn3l-kLP8mrz",
    "papermill": {
     "duration": 0.015853,
     "end_time": "2025-11-13T03:41:08.184941",
     "exception": false,
     "start_time": "2025-11-13T03:41:08.169088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render_builtin_reference(df):\n",
    "    return \"\\n\".join(f\"- {row['name']}\" for _, row in df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c53e93b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:08.195120Z",
     "iopub.status.busy": "2025-11-13T03:41:08.195120Z",
     "iopub.status.idle": "2025-11-13T03:41:08.203858Z",
     "shell.execute_reply": "2025-11-13T03:41:08.203858Z"
    },
    "id": "8QJimfY-5Tef",
    "outputId": "6364d500-e5d0-4675-8d5f-dd010a6986c6",
    "papermill": {
     "duration": 0.018917,
     "end_time": "2025-11-13T03:41:08.203858",
     "exception": false,
     "start_time": "2025-11-13T03:41:08.184941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in metadata used in the Contexts and Dependency Injection (CDI) library/framework.\n",
      "Here is the core syntax of the language you will translate in plain English natural language per rule. This is the core syntax of Rule-Specific Language (RSL): Specification := Rule Id Body\n",
      "Body := '{' Stmt Stmt* '}'\n",
      "Stmt := ForStmt | IfStmt | AssertStmt | DeclStmt ';'\n",
      "\n",
      "ForStmt := 'for' '(' Type Id 'in' Exp ')' Body\n",
      "IfStmt := 'if' '(' Exp ')' Body\n",
      "\n",
      "AssertStmt := 'assert' '(' Exp ')' '{' MsgStmt ';' '}'\n",
      "MsgStmt := 'msg' '(' ',' SimExp (',' SimExp)* ')'\n",
      "\n",
      "DeclStmt := Type Id '=' Exp\n",
      "\n",
      "Exp := SimExp\n",
      "     | SimExp AND Exp\n",
      "     | SimExp OR  Exp\n",
      "     | NOT Exp\n",
      "\n",
      "SimExp := Id\n",
      "        | Lit\n",
      "        | FunctionCall\n",
      "        | '(' Exp ')'\n",
      "        | FunctionCall '==' SimExp\n",
      "        | exists '(' Type Id in Exp ')' '(' Exp ')'\n",
      "\n",
      "Type := '⟨' Id '⟩' | file | class | method | field | String\n",
      "Lit := StringLit | CharLit | IntLit | FloatLit\n",
      "FunctionCall := Id '(' Params ')'\n",
      "Params := SimExp (',' SimExp)*\n",
      "\n",
      "Here is the currently built-in functions: - callExists\n",
      "- classExists\n",
      "- getArg\n",
      "- getClasses\n",
      "- getConstructors\n",
      "- getFamily\n",
      "- getFields\n",
      "- getFQN\n",
      "- getMethods\n",
      "- getName\n",
      "- getReturnType\n",
      "- getSN\n",
      "- getType\n",
      "- hasField\n",
      "- hasParam\n",
      "- hasParamType\n",
      "- indexInBound\n",
      "- isIterable\n",
      "- isLibraryClass\n",
      "- isUniqueSN\n",
      "- locateClassSN\n",
      "- locateClassFQN\n",
      "- getAnnoAttr\n",
      "- getAnnoAttrNames\n",
      "- getAnnotated\n",
      "- hasAnnotation\n",
      "- hasAnnoAttr\n",
      "- elementExists\n",
      "- getAttr\n",
      "- getAttrs\n",
      "- getElms\n",
      "- getXMLs\n",
      "- hasAttr\n",
      "- endsWith\n",
      "- isEmpty\n",
      "- indexOf\n",
      "- join\n",
      "- pathExists\n",
      "- substring\n",
      "- startsWith\n",
      "- upperCase\n",
      "Here are three example rules expressed in RSL: // Rule 1 — bean-class-exists\n",
      "Rule bean-class-exists {\n",
      "  for (file xml in getXMLs()) {\n",
      "    if (elementExists(xml, \"<bean>\")) {\n",
      "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
      "        String beanClassFQN = getAttr(bean, \"class\");\n",
      "        if (NOT isEmpty(beanClassFQN)) {\n",
      "          assert ( classExists(beanClassFQN) OR isLibraryClass(beanClassFQN) ) {\n",
      "            msg(\"Bean class: %s mentioned in bean: %s, does not exist\",\n",
      "                beanClassFQN, getName(bean));\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "// Rule 3 — constructor-arg-name-field-map\n",
      "Rule constructor-arg-name-field-map {\n",
      "  for (file xml in getXMLs()) {\n",
      "    if (elementExists(xml, \"<bean>\")) {\n",
      "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
      "        String beanClassFQN = getAttr(bean, \"class\");\n",
      "        if (classExists(beanClassFQN)) {\n",
      "          class c = locateClassFQN(beanClassFQN);\n",
      "          for (<constructor-arg> constructor_arg in getElms(bean, \"<constructor-arg>\")) {\n",
      "            String arg_name = getAttr(constructor_arg, \"name\");\n",
      "            if (NOT isEmpty(arg_name)) {\n",
      "              assert ( exists(method con in getConstructors(c)) ( hasParam(con, arg_name) ) ) {\n",
      "                msg(\"The name of <constructor-arg>: %s in bean: %s does not correspond to any constructor parameter in class: %s\",\n",
      "                    arg_name, getName(bean), getFQN(c));\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "// Rule 5 — constructor-index-out-of-bound\n",
      "Rule constructor-index-out-of-bound {\n",
      "  for (file xml in getXMLs()) {\n",
      "    if (elementExists(xml, \"<bean>\")) {\n",
      "      for (<bean> bean in getElms(xml, \"<bean>\")) {\n",
      "        String beanClassFQN = getAttr(bean, \"class\");\n",
      "        if (classExists(beanClassFQN)) {\n",
      "          class c = locateClassFQN(beanClassFQN);\n",
      "          for (<constructor-arg> constructor_arg in getElms(bean, \"<constructor-arg>\")) {\n",
      "            if (hasAttr(constructor_arg, \"index\")) {\n",
      "              String arg_idx = getAttr(constructor_arg, \"index\");\n",
      "              assert ( exists(method constructor in getConstructors(c)) ( indexInBound(constructor, arg_idx) ) ) {\n",
      "                msg(\"Constructor index: %s of bean for class: %s in xml: %s is out of bound\",\n",
      "                    arg_idx, getFQN(c), getName(xml));\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "As sample rules, I shared three rules with you.\n",
      "Your job is to create one rule that corresponds to the topic Incorrect bean definitions in beans.xml﻿ with the following content: Reports incorrect bean definitions in beans.xml configuration files..\n",
      "\n",
      "The JetBrains Inspectopedia source page for this rule is: https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html.\n",
      "For the field \"source URL (JetBrains web page)\", output exactly https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html.\n",
      "\n",
      "Like the sample rules, it should be described following the syntax of RSL. You briefly describe what the new rule checks. The new rule must use only the built-in functions. This means that you should not introduce new built-in functions while creating a new rule corresponding to the topic and content. In the \"new_built-in_functions_explanation if any\" field, provide names of all built-in functions used in the new rule after confirming that no new functions were introduced. Also, there should not be any comments or BOM (byte order mark) to save the rule in a txt file.\n",
      "\n",
      "Then, locate the most relevant, at most, two 3rd-party web pages that address the the topic, content, and the generated rule in terms of metadata-related bugs. Provide a brief summary per the 3rd-party web page content in one field together. Finally, provide a post-model validation based on generated rule and the located most relevant 3rd-party web page(s).\n",
      "Prioritize finding 3rd-party web pages with at least one of the following keywords in the URLs to ensure relevancy: [\"CDI\", \"beans.xml\", \"IncorrectBeanDefinitions\"]. The keywords found in the URLs do not have to be exact matches to the keywords in the list.\n",
      "\n",
      "Return the output strictly as a valid JSON array, not text, not markdown, not explanation.\n",
      "You must NOT include any text, commentary, or code fences (like ```).\n",
      "You must NOT prepend or append any text before or after the JSON array.\n",
      "\n",
      "Each JSON object must include exactly these fields:\n",
      "[\n",
      "  \"framework\",\n",
      "  \"source URL (JetBrains web page)\",\n",
      "  \"brief_description of the content in the source URL\",\n",
      "  \"GPT-4 generated_rule\",\n",
      "  \"generated_rule_explanation from GPT-4\",\n",
      "  \"new_built-in_functions_explanation if any\",\n",
      "  \"PRE_model_validation\",\n",
      "  \"3rd-party most relevant URLs and summary for each URL\",\n",
      "  \"POST_model_validation\"\n",
      "]\n",
      "\n",
      "For the \"brief_description of the content in the source URL\" field, provide a concise summary of the content provided in the prompt (Reports incorrect bean definitions in beans.xml configuration files.), which comes from the JetBrains page at https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html. Do NOT describe any other URL.\n",
      "\n",
      "The output must start with \"[\" and end with \"]\".\n",
      "If you cannot find information for any field, use an empty string (\"\").\n"
     ]
    }
   ],
   "source": [
    "def _mk_source_suffix(source: str | None) -> str:\n",
    "    if not source or str(source).strip() == \"\":\n",
    "        return \"\"\n",
    "    return f\" (Reference source: {source})\"  # ✅ For now source is hard-coded\n",
    "\n",
    "\n",
    "def render_original_prompt(\n",
    "    framework: str = FRAMEWORK,\n",
    "    num_rules: int = NUM_RULES,\n",
    "    topic: str = TOPIC,\n",
    "    source: str | None = SOURCE,\n",
    "    template_text: str = PROMPT_ORIGINAL_TMPL,\n",
    "    content: str = CONTENT,\n",
    "    keywords: str = \", \".join(KEYWORDS)   # ← 改为逗号分隔\n",
    "):\n",
    "    \"\"\"Splicing Final Prompt\"\"\"\n",
    "    return (template_text\n",
    "            .replace(\"{{RSL_SYNTAX}}\", RSL_SYNTAX)\n",
    "            .replace(\"{{EXAMPLE_RULES}}\", RSL_EXAMPLE_RULES)\n",
    "            .replace(\"{{BUILTIN_REFERENCE}}\", render_builtin_reference(builtins_df))\n",
    "            .replace(\"{{FRAMEWORK}}\", framework)\n",
    "\n",
    "            .replace(\"{{SOURCE_SUFFIX}}\", _mk_source_suffix(source))\n",
    "            .replace(\"{{SOURCE}}\", (source or \"\").strip())  \n",
    "            .replace(\"{{NUM_RULES}}\", str(num_rules))\n",
    "            .replace(\"{{TOPIC}}\", topic)\n",
    "            .replace(\"{{CONTENT}}\", content)\n",
    "            .replace(\"{{KEYWORDS}}\", keywords))\n",
    "\n",
    "\n",
    "# === [Block 2] Prompt Generation Combining All Info  ===\n",
    "final_prompt = render_original_prompt()\n",
    "\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005e2573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:08.211150Z",
     "iopub.status.busy": "2025-11-13T03:41:08.211150Z",
     "iopub.status.idle": "2025-11-13T03:41:08.220217Z",
     "shell.execute_reply": "2025-11-13T03:41:08.220217Z"
    },
    "papermill": {
     "duration": 0.009067,
     "end_time": "2025-11-13T03:41:08.220217",
     "exception": false,
     "start_time": "2025-11-13T03:41:08.211150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html\n"
     ]
    }
   ],
   "source": [
    "print(SOURCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e519113",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:08.227328Z",
     "iopub.status.busy": "2025-11-13T03:41:08.227328Z",
     "iopub.status.idle": "2025-11-13T03:41:47.686963Z",
     "shell.execute_reply": "2025-11-13T03:41:47.686963Z"
    },
    "id": "c6IlUuOmRvEM",
    "outputId": "c2110e75-2091-452d-ad85-f91498cfbc0b",
    "papermill": {
     "duration": 39.460739,
     "end_time": "2025-11-13T03:41:47.688067",
     "exception": false,
     "start_time": "2025-11-13T03:41:08.227328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned data preview:\n",
      " [\n",
      "  {\n",
      "    \"framework\": \"Contexts and Dependency Injection (CDI)\",\n",
      "    \"source URL (JetBrains web page)\": \"https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html\",\n",
      "    \"brief_description of the content in the source URL\": \"The JetBrains Inspectopedia page provides information about the CDI Dom Beans inspection, which reports incorrect bean definitions in beans.xml configuration files.\",\n",
      "    \"GPT-4 generated_rule\": \"Rule incorrect-bean-definitions {\\n  for (file xml in getXMLs()) {\n",
      "✅ Saved 1 rules to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\new_GPT_4_generated_rule.txt\n",
      "✅ Saved new_generated_rule_explanation to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\new_generated_rule_explanation.json\n",
      "✅ Saved new_generated_rule_explanation to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\new_generated_rule_explanation.json\n",
      "⚠️ No saved GPT-4 rule description found.\n",
      "\n",
      "--- Loaded back from JSON scraped from JetBrains  ---\n",
      "\n",
      " ...\n",
      "\n",
      "--- Loaded back from JSON from gpt4_rule_explanation ---\n",
      "\n",
      "This rule, named 'incorrect-bean-definitions', checks all XML files for bean elements. For each bean element, it retrieves the fully qualified name of the bean class. If the bean class name is not empty, it asserts that the class either exists or is a library class. If the assertion fails, it generates a message indicating an incorrect bean definition, specifying the fully qualified name of the bean class and the name of the bean. ...\n"
     ]
    }
   ],
   "source": [
    "# --- Config (local paths) ---\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "ARTIFACT_DIR = Path(ARTIFACTS_DIR)          \n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_PATH = str(ARTIFACT_DIR / \"original_rsl_generated_rules.csv\")\n",
    "newRULE_PATH = str(ARTIFACT_DIR / \"new_GPT_4_generated_rule.txt\")\n",
    "newRULE_EXPLANATIONS_PATH = str(ARTIFACT_DIR / \"new_generated_rule_explanation.json\")\n",
    "GPT4_RULE_DESCRIPTION_PATH = str(ARTIFACT_DIR / \"gpt4_rule_description_scraped_content.json\")\n",
    "\n",
    "\n",
    "\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"framework\",\n",
    "    \"source URL (JetBrains web page)\",\n",
    "    \"brief_description of the content in the source URL\",\n",
    "    \"GPT-4 generated_rule\",\n",
    "    \"generated_rule_explanation from GPT-4\",\n",
    "    \"new_built-in_functions_explanation if any\",\n",
    "    \"PRE_model_validation\",\n",
    "    \"3rd-party most relevant URLs and summary for each URL\",\n",
    "    \"POST_model validation\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- OpenAI client (no explicit temperature; model default applies) ---\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found. Put it in your .env or system env.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"user\", \"content\": final_prompt}]\n",
    ")\n",
    "\n",
    "raw = response.choices[0].message.content.strip()\n",
    "print(\"Returned data preview:\\n\", raw[:500])\n",
    "rules = json.loads(raw)\n",
    "\n",
    "# --- Normalize to your schema (fill missing fields with empty strings) ---\n",
    "def coerce_to_schema(item: dict) -> dict:\n",
    "    # Try to map common alternative keys if your prompt returns slightly different names\n",
    "    aliases = {\n",
    "        \"framework\": [\"framework\"],\n",
    "        \"source URL (JetBrains web page)\": [\"source_url\", \"source URL\", \"jetbrains_source\", \"jetbrains_url\"],\n",
    "        \"brief_description of the content in the source URL\": [\"brief_description\", \"brief description\", \"source_brief\"],\n",
    "        \"GPT-4 generated_rule\": [\"generated_rule\", \"rule\", \"gpt4_rule\"],\n",
    "        \"generated_rule_explanation from GPT-4\": [\"generated_rule_explanation\", \"rule_explanation\", \"gpt4_rule_explanation\"],\n",
    "        \"new_built-in_functions_explanation if any\": [\"new_built_in_functions_explanation\", \"new_built-in_functions_explanation\", \"new_functions_note\"],\n",
    "        \"PRE_model_validation\": [\"pre_model_validation\", \"pre_validation\"],\n",
    "        \"3rd-party most relevant URLs and summary for each URL\": [\"third_party_urls_and_summaries\", \"3rd_party_urls_summaries\"],\n",
    "        \"POST_model validation\": [\"post_model_validation\", \"post_validation\"],\n",
    "    }\n",
    "\n",
    "    out = {}\n",
    "    for col in EXPECTED_COLUMNS:\n",
    "        val = \"\"\n",
    "        if isinstance(item, dict):\n",
    "            if col in item:\n",
    "                val = item[col]\n",
    "            else:\n",
    "                # look for an alias key\n",
    "                for alt in aliases.get(col, []):\n",
    "                    if alt in item:\n",
    "                        val = item[alt]\n",
    "                        break\n",
    "        out[col] = val if val is not None else \"\"\n",
    "    return out\n",
    "\n",
    "rows = [coerce_to_schema(x if isinstance(x, dict) else {}) for x in (rules if isinstance(rules, list) else [rules])]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=EXPECTED_COLUMNS)\n",
    "\n",
    "# --- Save column “GPT-4 generated_rule” to TXT (one per line) ---\n",
    "generated_rules_txt = [str(x).strip() for x in df[\"GPT-4 generated_rule\"].fillna(\"\")]\n",
    "with open(newRULE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in generated_rules_txt:\n",
    "        if line:\n",
    "            f.write(line + \"\\n\")\n",
    "print(f\"✅ Saved {len([x for x in generated_rules_txt if x])} rules to {newRULE_PATH}\")\n",
    "\n",
    "# --- Save column “generated_rule_explanation from GPT-4” to JSON (list of strings) ---\n",
    "\n",
    "generated_explanations = [str(x).strip() for x in df[\"generated_rule_explanation from GPT-4\"].fillna(\"\") if str(x).strip()]\n",
    "single_explanation = generated_explanations[0] if generated_explanations else \"\"\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(newRULE_EXPLANATIONS_PATH), exist_ok=True)\n",
    "    with open(newRULE_EXPLANATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\"new_generated_rule_explanation\": single_explanation},\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "    print(f\"✅ Saved new_generated_rule_explanation to {newRULE_EXPLANATIONS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save JSON: {e}\")\n",
    "\n",
    "with open(newRULE_EXPLANATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generated_explanations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(newRULE_EXPLANATIONS_PATH), exist_ok=True)\n",
    "    with open(newRULE_EXPLANATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\"new_generated_rule_explanation\": single_explanation},\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2\n",
    "        )\n",
    "    print(f\"✅ Saved new_generated_rule_explanation to {newRULE_EXPLANATIONS_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save JSON: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_new_rule_explanation(path: str = GPT4_RULE_DESCRIPTION_PATH) -> str:\n",
    "    \"\"\"\n",
    "    Loads the previously saved GPT-5 rule description for contextual use in SerpAPI searches.\n",
    "    Returns an empty string if none exists or file is corrupted.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"⚠️ No saved GPT-4 rule description found.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data.get(\"jetbrains_scraped_rule_description\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load saved description: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_gpt4_rule_description(path: str = newRULE_EXPLANATIONS_PATH) -> str:\n",
    "    \"\"\"\n",
    "    Loads the new-GPT-4-generated rule explanation to provide context to SerpAPI later.\n",
    "    Returns an empty string if none exists or file is corrupted.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(\"⚠️ No new-GPT-4 rule explanation found.\")\n",
    "        return \"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data.get(\"new_generated_rule_explanation\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load saved description: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Step 2: Load later (e.g., in a new session)\n",
    "loaded_text = load_new_rule_explanation(GPT4_RULE_DESCRIPTION_PATH)\n",
    "print(\"\\n--- Loaded back from JSON scraped from JetBrains  ---\\n\")\n",
    "print(loaded_text[:500], \"...\")\n",
    "\n",
    "# Step 3: Load later (e.g., in a new session)\n",
    "loaded_text1 = load_gpt4_rule_description(newRULE_EXPLANATIONS_PATH)\n",
    "print(\"\\n--- Loaded back from JSON from gpt4_rule_explanation ---\\n\")\n",
    "print(loaded_text1[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eda455d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:47.703850Z",
     "iopub.status.busy": "2025-11-13T03:41:47.703850Z",
     "iopub.status.idle": "2025-11-13T03:41:47.722453Z",
     "shell.execute_reply": "2025-11-13T03:41:47.722453Z"
    },
    "id": "MZdAfjRcZt6g",
    "outputId": "632037c3-75e8-4230-80e7-71cb5a765cff",
    "papermill": {
     "duration": 0.020211,
     "end_time": "2025-11-13T03:41:47.724061",
     "exception": false,
     "start_time": "2025-11-13T03:41:47.703850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved normalized data to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\original_rsl_generated_rules.csv\n"
     ]
    }
   ],
   "source": [
    "#--- Finally, save the full normalized CSV ---\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "print(f\"✅ Saved normalized data to {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e4be06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:47.737390Z",
     "iopub.status.busy": "2025-11-13T03:41:47.737390Z",
     "iopub.status.idle": "2025-11-13T03:41:47.791294Z",
     "shell.execute_reply": "2025-11-13T03:41:47.791294Z"
    },
    "id": "c_Dxur6iZXgO",
    "outputId": "e63c1902-b4c3-42d6-b964-830ba6528087",
    "papermill": {
     "duration": 0.067233,
     "end_time": "2025-11-13T03:41:47.791294",
     "exception": false,
     "start_time": "2025-11-13T03:41:47.724061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2 URLs to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\gpt4_found_urls.json\n",
      "🧱 EXCLUDED_DOMAINS (static) = {'example.org', 'jetbrains.com.cn', 'www.jetbrains.com.cn', 'jetbrains.com', 'www.jetbrains.com', 'example.com'}\n"
     ]
    }
   ],
   "source": [
    "# Paths & column name that should be excluded for SerpAPI search\n",
    "CSV_URLS_COLUMN = \"3rd-party most relevant URLs and summary for each URL\"\n",
    "\n",
    "EXCLUDED_DOMAINS = {\n",
    "    \"jetbrains.com\",\n",
    "    \"www.jetbrains.com\",\n",
    "    \"jetbrains.com.cn\",\n",
    "    \"www.jetbrains.com.cn\",\n",
    "    \"example.com\",\n",
    "    \"example.org\"\n",
    "}\n",
    "\n",
    "from pathlib import Path\n",
    "ARTIFACT_DIR = Path(ARTIFACTS_DIR)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GPT_URLS_STORE = str(ARTIFACT_DIR / \"gpt4_found_urls.json\")\n",
    "\n",
    "\n",
    "# URL extractor\n",
    "_URL_RE = re.compile(r\"https?://[^\\s)>\\]}\\\"']+\", re.IGNORECASE)\n",
    "\n",
    "def _filter_real_urls(urls: Set[str]) -> Set[str]:\n",
    "    \"\"\"Remove JetBrains and example/test domains.\"\"\"\n",
    "    cleaned = set()\n",
    "    for url in urls:\n",
    "        lowered = url.lower()\n",
    "        if any(dom in lowered for dom in EXCLUDED_DOMAINS):\n",
    "            continue\n",
    "        cleaned.add(url.strip())\n",
    "    return cleaned\n",
    "\n",
    "def extract_urls(text: str) -> Set[str]:\n",
    "    return set(_URL_RE.findall(text or \"\"))\n",
    "\n",
    "def load_urls_from_csv(csv_path: str = CSV_PATH,\n",
    "                       column: str = CSV_URLS_COLUMN) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Reads the CSV at csv_path and extracts all URLs from the target column.\n",
    "    Returns a set of URLs (may be empty if file/column missing).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read CSV at {csv_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "    if column not in df.columns:\n",
    "        print(f\"⚠️ Column not found: '{column}'. Available: {list(df.columns)}\")\n",
    "        return set()\n",
    "\n",
    "    urls: Set[str] = set()\n",
    "    for cell in df[column].dropna().astype(str):\n",
    "        urls |= extract_urls(cell)\n",
    "    return urls\n",
    "\n",
    "def load_gpt5_found_urls(path: str = GPT_URLS_STORE,\n",
    "                         csv_path: str = CSV_PATH,\n",
    "                         column: str = CSV_URLS_COLUMN) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Loads URLs previously saved in JSON and merges with URLs extracted\n",
    "    from the CSV column.\n",
    "    \"\"\"\n",
    "    merged: Set[str] = set()\n",
    "\n",
    "    # 1) URLs from CSV\n",
    "    csv_urls = load_urls_from_csv(csv_path, column)\n",
    "    if csv_urls:\n",
    "        merged |= csv_urls\n",
    "\n",
    "    # 2) URLs from JSON store (if present)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                merged |= set(map(str, data))\n",
    "            else:\n",
    "                print(\"⚠️ JSON store not a list; ignoring its contents.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load {path}: {e}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def save_gpt5_found_urls(\n",
    "    urls: Set[str],\n",
    "    path: str = GPT_URLS_STORE,\n",
    "    also_merge_csv: bool = True,\n",
    "    csv_path: str = CSV_PATH,\n",
    "    column: str = CSV_URLS_COLUMN\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves the union of:\n",
    "      - provided `urls`\n",
    "      - existing JSON store\n",
    "      - (optionally) URLs parsed from the CSV column\n",
    "    \"\"\"\n",
    "    # 1️⃣ Merge URLs\n",
    "    existing = load_gpt5_found_urls(path, csv_path, column) if also_merge_csv else set()\n",
    "    valid_urls = _filter_real_urls(urls)\n",
    "    merged = sorted(existing | valid_urls)\n",
    "\n",
    "    # 2️⃣ Save merged URLs to file\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"✅ Saved {len(merged)} URLs to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save {path}: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # before_count = len(EXCLUDED_DOMAINS)\n",
    "    # EXCLUDED_DOMAINS.update(existing)\n",
    "    # after_count = len(EXCLUDED_DOMAINS)\n",
    "    # print(f\"🔒 Updated EXCLUDED_DOMAINS: added {after_count - before_count} new URL(s).\")\n",
    "    print(f\"🧱 EXCLUDED_DOMAINS (static) = {EXCLUDED_DOMAINS}\")\n",
    "\n",
    "# 1) Pull URLs from CSV + existing JSON\n",
    "seed_urls = load_gpt5_found_urls()\n",
    "\n",
    "# 2) Add any new URLs (e.g., from a fresh GPT-4 output or another pass)\n",
    "new_urls = {\"https://example.com/a\", \"https://example.org/b\"}\n",
    "save_gpt5_found_urls(new_urls)   # merges CSV + existing JSON + new ones into the store\n",
    "\n",
    "# 3) Later: build your exclusion set from this store for SerpAPI searches\n",
    "gpt5_found_urls = load_gpt5_found_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51972197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:47.799027Z",
     "iopub.status.busy": "2025-11-13T03:41:47.799027Z",
     "iopub.status.idle": "2025-11-13T03:41:47.832969Z",
     "shell.execute_reply": "2025-11-13T03:41:47.832969Z"
    },
    "id": "casy8sRwNbMl",
    "papermill": {
     "duration": 0.033942,
     "end_time": "2025-11-13T03:41:47.832969",
     "exception": false,
     "start_time": "2025-11-13T03:41:47.799027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "DEFAULT_EXCLUDES = {\n",
    "    \"jetbrains.com\",\n",
    "    \"www.jetbrains.com\",\n",
    "    \"jetbrains.com.cn\",\n",
    "    \"www.jetbrains.com.cn\",\n",
    "}\n",
    "\n",
    "# More automation number 5\n",
    "HOST_PRIORITY: Dict[str, int] = {\n",
    "    \"docs.jboss.org\": 100,             # Hibernate javadocs\n",
    "    \"spring.io\": 95,               # Hibernate website\n",
    "    \"jakarta.ee\": 80,                  # EE docs\n",
    "    \"jetbrains.com\": 70,               # Inspectopedia/IDEA docs\n",
    "    \"github.com\": 40,                  # Code often useful\n",
    "}\n",
    "\n",
    "def _fetch_text(url: str, timeout: int = 20) -> str:\n",
    "    resp = requests.get(url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for tag in soup([\"script\",\"style\",\"noscript\"]):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "def search_framework_find(framework: str, topic: str,\n",
    "                          jetbrains_intro: str, gpt4_expl: str,\n",
    "                          exclude_domains: Optional[Set[str]] = None,\n",
    "                          per_query: int = 10) -> List[dict]:\n",
    "    queries = build_candidate_queries(framework, topic, jetbrains_intro, gpt4_expl)\n",
    "\n",
    "    all_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        batch = serpapi_search(q, exclude_domains=exclude_domains, num=per_query)\n",
    "        all_results.extend(batch)\n",
    "        time.sleep(0.2)  # be polite; SerpAPI handles rate limiting but avoid bursts\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup: Dict[str, dict] = {}\n",
    "    for r in all_results:\n",
    "        dedup[r[\"url\"]] = r\n",
    "    ranked = rerank(list(dedup.values()))\n",
    "    return ranked[:20]  # top-N\n",
    "\n",
    "def serpapi_search(query: str, exclude_domains: Optional[Set[str]] = None, num: int = 10) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Returns a *list of result dicts* (we keep title/snippet for reranking).\n",
    "    \"\"\"\n",
    "    if not SERPAPI_API_KEY:\n",
    "        print(\"⚠️ No SerpAPI key provided.\")\n",
    "        return []\n",
    "\n",
    "    excludes = set(DEFAULT_EXCLUDES)\n",
    "    if exclude_domains:\n",
    "        excludes |= set(exclude_domains)\n",
    "\n",
    "    params = {\"engine\": \"google\", \"q\": query, \"api_key\": SERPAPI_API_KEY, \"num\": str(num)}\n",
    "    try:\n",
    "        r = requests.get(\"https://serpapi.com/search\", params=params, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        out = []\n",
    "        for item in data.get(\"organic_results\", []):\n",
    "            url = item.get(\"link\")\n",
    "            if not url:\n",
    "                continue\n",
    "            if is_excluded(url, excludes):\n",
    "                continue\n",
    "            out.append({\n",
    "                \"url\": url,\n",
    "                \"title\": item.get(\"title\", \"\"),\n",
    "                \"snippet\": item.get(\"snippet\", \"\"),\n",
    "                \"position\": item.get(\"position\"),\n",
    "            })\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ SerpAPI request failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def is_excluded(url: str, excluded_domains: Set[str]) -> bool:\n",
    "    \"\"\"Return True if the URL's host matches any excluded domain.\"\"\"\n",
    "    if not excluded_domains:\n",
    "        return False\n",
    "    host = _extract_domain(url)\n",
    "    return any(host == dom or host.endswith(f\".{dom}\") for dom in excluded_domains)\n",
    "\n",
    "def build_candidate_queries(framework: str, topic: str, jetbrains_intro: str, gpt5_expl: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate several strong Google queries with operators. We’ll try them all and then rerank results.\n",
    "    \"\"\"\n",
    "    ctx = normalize_space(\" \".join([framework or \"\", topic or \"\", jetbrains_intro or \"\", gpt5_expl or \"\"]))\n",
    "    terms = key_terms_from_text(ctx)\n",
    "    joined = \" \".join(terms) if terms else ctx\n",
    "\n",
    "    # Pay special attention to '@Find' literal\n",
    "    base = '\"@PathVariable\" annotation attribute spring mvc'\n",
    "\n",
    "    # More automation number 1.\n",
    "    candidates = [\n",
    "        # precision: official javadocs\n",
    "        f'site:docs.jboss.org {base}',\n",
    "        f'site:spring.io {base}',\n",
    "        # title/url hints\n",
    "        f'intitle:{base}',\n",
    "        f'inurl:spring intitle:PathVariable',\n",
    "        # allintext to bind concepts\n",
    "        f'allintext:PathVariable spring {joined}',\n",
    "        # combine with JetBrains if trying to verify Inspectopedia rule\n",
    "        f'site:jetbrains.com Inspectopedia \"PathVariable Spring MVC',\n",
    "        # fallback broad\n",
    "        f'{base} {joined}',\n",
    "    ]\n",
    "\n",
    "    # de-duplicate + keep short, valid strings\n",
    "    return uniq_preserve_order([normalize_space(c) for c in candidates if c.strip()])\n",
    "\n",
    "def _extract_domain(url: str) -> str:\n",
    "    try:\n",
    "        host = re.sub(r\"^https?://\", \"\", url, flags=re.IGNORECASE).split(\"/\")[0]\n",
    "        return host.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def key_terms_from_text(text: str, k: int = 8) -> List[str]:\n",
    "    \"\"\"\n",
    "    Cheap, dependency-free keyterm picker: keep tokens with @, camelCase, or java-ish/Dot terms; fallback to frequent words.\n",
    "    Replace with RAKE/TextRank if you like.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    toks = re.findall(r\"[@\\w\\.]+\", text)\n",
    "    # keep distinctive tokens\n",
    "    candidates = [t for t in toks if len(t) > 2]\n",
    "    # light heuristic boosts\n",
    "    def score(t: str) -> float:\n",
    "        s = 0.0\n",
    "        if t.startswith(\"@\"): s += 3\n",
    "        if \".\" in t: s += 1.5   # package/class names\n",
    "        if re.search(r\"[A-Z][a-z]+[A-Z]\", t): s += 1.0  # camelCase\n",
    "\n",
    "        # More automation number 2.\n",
    "        if t.lower() in {\"pathvariable\",\"requestmapping\",\"spring\",\"springmvc\",\"incorrect\",\"mismatch\"}:\n",
    "            s += 1.2\n",
    "        return s\n",
    "    ranked = sorted(candidates, key=lambda t: (score(t), len(t)), reverse=True)\n",
    "    out = uniq_preserve_order(ranked)[:k]\n",
    "    return out\n",
    "\n",
    "def uniq_preserve_order(seq: Iterable[str]) -> List[str]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for s in seq:\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "def rerank(results: List[dict]) -> List[dict]:\n",
    "    return sorted(results, key=feature_score, reverse=True)\n",
    "\n",
    "# More automation number 3\n",
    "def feature_score(result: dict) -> float:\n",
    "    \"\"\"\n",
    "    Score by: host priority, presence of '@Find' in title/snippet/url,\n",
    "    path hints like '/annotations/processing/Find'.\n",
    "    \"\"\"\n",
    "    url, title, snip = result.get(\"url\",\"\"), result.get(\"title\",\"\"), result.get(\"snippet\",\"\")\n",
    "    host = host_of(url)\n",
    "    score = HOST_PRIORITY.get(host, 0)\n",
    "\n",
    "    text = \" \".join([url, title, snip]).lower()\n",
    "    boosts = [(word, score) for word, score in zip(KEYWORDS, SCORES)]\n",
    "    for pat, w in boosts:\n",
    "        if re.search(pat, text):\n",
    "            score += w\n",
    "\n",
    "    # slight boost for earlier rank\n",
    "    pos = result.get(\"position\")\n",
    "    if isinstance(pos, int):\n",
    "        score += max(0, 10 - pos)\n",
    "\n",
    "    return float(score)\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "def host_of(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def is_excluded(url: str, excluded_hosts: Set[str]) -> bool:\n",
    "    h = host_of(url)\n",
    "    return any(h == ex or h.endswith(f\".{ex}\") for ex in excluded_hosts)\n",
    "\n",
    "def uniq_preserve_order(seq: Iterable[str]) -> List[str]:\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for s in seq:\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Query building\n",
    "# -------------------------------\n",
    "# def normalize_space(s: str) -> str:\n",
    "#     return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "# def key_terms_from_text(text: str, k: int = 8) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Cheap, dependency-free keyterm picker: keep tokens with @, camelCase, or java-ish/Dot terms; fallback to frequent words.\n",
    "#     Replace with RAKE/TextRank if you like.\n",
    "#     \"\"\"\n",
    "#     if not text:\n",
    "#         return []\n",
    "#     toks = re.findall(r\"[@\\w\\.]+\", text)\n",
    "#     # keep distinctive tokens\n",
    "#     candidates = [t for t in toks if len(t) > 2]\n",
    "#     # light heuristic boosts\n",
    "#     def score(t: str) -> float:\n",
    "#         s = 0.0\n",
    "#         if t.startswith(\"@\"): s += 3\n",
    "#         if \".\" in t: s += 1.5   # package/class names\n",
    "#         if re.search(r\"[A-Z][a-z]+[A-Z]\", t): s += 1.0  # camelCase\n",
    "#         if t.lower() in {\"find\",\"finder\",\"annotation\",\"hibernate\",\"entity\",\"typedquery\",\"selectionquery\"}:\n",
    "#             s += 1.2\n",
    "#         return s\n",
    "#     ranked = sorted(candidates, key=lambda t: (score(t), len(t)), reverse=True)\n",
    "#     out = uniq_preserve_order(ranked)[:k]\n",
    "#     return out\n",
    "\n",
    "# def build_candidate_queries(framework: str, topic: str, jetbrains_intro: str, gpt5_expl: str) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Generate several strong Google queries with operators. We’ll try them all and then rerank results.\n",
    "#     \"\"\"\n",
    "#     ctx = normalize_space(\" \".join([framework or \"\", topic or \"\", jetbrains_intro or \"\", gpt5_expl or \"\"]))\n",
    "#     terms = key_terms_from_text(ctx)\n",
    "#     joined = \" \".join(terms) if terms else ctx\n",
    "\n",
    "#     # Pay special attention to '@Find' literal\n",
    "#     base = '\"@Find\" hibernate annotation'\n",
    "\n",
    "#     candidates = [\n",
    "#         # precision: official javadocs\n",
    "#         f'site:docs.jboss.org {base}',\n",
    "#         f'site:hibernate.org {base}',\n",
    "#         # title/url hints\n",
    "#         f'intitle:@Find hibernate annotation',\n",
    "#         f'inurl:hibernate intitle:@Find annotation',\n",
    "#         # allintext to bind concepts\n",
    "#         f'allintext:@Find hibernate entity finder {joined}',\n",
    "#         # combine with JetBrains if trying to verify Inspectopedia rule\n",
    "#         f'site:jetbrains.com Inspectopedia \"@Find\" hibernate',\n",
    "#         # fallback broad\n",
    "#         f'{base} {joined}',\n",
    "#     ]\n",
    "\n",
    "#     # de-duplicate + keep short, valid strings\n",
    "#     return uniq_preserve_order([normalize_space(c) for c in candidates if c.strip()])\n",
    "\n",
    "# -------------------------------\n",
    "# Lightweight reranking\n",
    "# # -------------------------------\n",
    "# def feature_score(result: dict) -> float:\n",
    "#     \"\"\"\n",
    "#     Score by: host priority, presence of '@Find' in title/snippet/url,\n",
    "#     path hints like '/annotations/processing/Find'.\n",
    "#     \"\"\"\n",
    "#     url, title, snip = result.get(\"url\",\"\"), result.get(\"title\",\"\"), result.get(\"snippet\",\"\")\n",
    "#     host = host_of(url)\n",
    "#     score = HOST_PRIORITY.get(host, 0)\n",
    "\n",
    "#     text = \" \".join([url, title, snip]).lower()\n",
    "\n",
    "#     # boosts = [\n",
    "#     #     (r\"@configmapping\", 15),\n",
    "#     #     (r\"\\bquarkus\\b\", 8),\n",
    "#     #     (r\"\\bannotation(s)?\\b\", 5),\n",
    "#     #     (r\".io/smallrye-config/\", 12),\n",
    "#     #     (r\"/smallrye/\", 10),\n",
    "#     #     (r\"\\bprefix\\b\", 4),\n",
    "#     # ]\n",
    "#     for pat, w in boosts:\n",
    "#         if re.search(pat, text):\n",
    "#             score += w\n",
    "\n",
    "#     # slight boost for earlier rank\n",
    "#     pos = result.get(\"position\")\n",
    "#     if isinstance(pos, int):\n",
    "#         score += max(0, 10 - pos)\n",
    "\n",
    "#     return float(score)\n",
    "\n",
    "def rerank(results: List[dict]) -> List[dict]:\n",
    "    return sorted(results, key=feature_score, reverse=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Orchestrator\n",
    "# -------------------------------\n",
    "def search__find(framework: str, topic: str,\n",
    "                          jetbrains_intro: str, gpt4_expl: str,\n",
    "                          exclude_domains: Optional[Set[str]] = None,\n",
    "                          per_query: int = 10) -> List[dict]:\n",
    "    queries = build_candidate_queries(framework, topic, jetbrains_intro, gpt4_expl)\n",
    "\n",
    "    all_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        batch = serpapi_search(q, exclude_domains=exclude_domains, num=per_query)\n",
    "        all_results.extend(batch)\n",
    "        time.sleep(0.2)  # be polite; SerpAPI handles rate limiting but avoid bursts\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup: Dict[str, dict] = {}\n",
    "    for r in all_results:\n",
    "        dedup[r[\"url\"]] = r\n",
    "    ranked = rerank(list(dedup.values()))\n",
    "    return ranked[:20]  # top-N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cb00716",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:41:47.847092Z",
     "iopub.status.busy": "2025-11-13T03:41:47.847092Z",
     "iopub.status.idle": "2025-11-13T03:43:31.148477Z",
     "shell.execute_reply": "2025-11-13T03:43:31.148477Z"
    },
    "id": "pqUgLFiwNOks",
    "outputId": "63401abd-72b8-4cc3-934d-c2c16d7a13d0",
    "papermill": {
     "duration": 103.315508,
     "end_time": "2025-11-13T03:43:31.148477",
     "exception": false,
     "start_time": "2025-11-13T03:41:47.832969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=CDI+beans.xml+alternatives+%3Cclass%3E+fully+qualified+name+exists&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen URL: https://docs.jboss.org/weld/reference/latest/en-US/html/configure.html#_beans_xml\n",
      "Model rationale:\n",
      " https://docs.jboss.org/weld/reference/latest/en-US/html/configure.html#_beans_xml\n",
      "\n",
      "Why it’s relevant: This Weld reference page explains the beans.xml structure and that class entries (e.g., for alternatives/interceptors/decorators) use fully qualified class names that must correspond to existing classes, matching what the incorrect-bean-definitions rule checks.\n",
      "✅ Saved 3 URLs to C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml\\gpt4_found_urls.json\n",
      "🧱 EXCLUDED_DOMAINS (static) = {'example.org', 'jetbrains.com.cn', 'www.jetbrains.com.cn', 'jetbrains.com', 'www.jetbrains.com', 'example.com'}\n",
      "⚠️ No saved GPT-4 rule description found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=site%3Adocs.jboss.org+%22%40PathVariable%22+annotation+attribute+spring+mvc&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=site%3Aspring.io+%22%40PathVariable%22+annotation+attribute+spring+mvc&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=intitle%3A%22%40PathVariable%22+annotation+attribute+spring+mvc&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=inurl%3Aspring+intitle%3APathVariable&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=allintext%3APathVariable+spring+beans.xml+elements.+class.+bean.+Incorrect+incorrect+definitions+Dependency&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=site%3Ajetbrains.com+Inspectopedia+%22PathVariable+Spring+MVC&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed: 429 Client Error: Too Many Requests for url: https://serpapi.com/search?engine=google&q=%22%40PathVariable%22+annotation+attribute+spring+mvc+beans.xml+elements.+class.+bean.+Incorrect+incorrect+definitions+Dependency&api_key=7d0961db7173e1210a6c719fff5e1a4a785c071ca300e2676cdd7f2b3e2b61b6&num=10\n"
     ]
    }
   ],
   "source": [
    "# --- Prereqs --------------------------------------------------------------\n",
    "# - serpapi_search() helper OR inline SerpAPI call\n",
    "# - SOURCE / FRAMEWORK / TOPIC and your intro loader\n",
    "# --- THESE TWO APPROACHES WORKED --------------------------------------------------------------\n",
    "\n",
    "from openai import OpenAI\n",
    "import os, json, re\n",
    "from urllib.parse import urlparse  \n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "\n",
    "# A thin wrapper that also enforces your exclusions.\n",
    "def serpapi_search_excluding(q: str, exclude: set, num: int = 10) -> list[str]:\n",
    "\n",
    "    merged_excludes = set(exclude) | {\n",
    "        \"jetbrains.com\", \"www.jetbrains.com\",\n",
    "        \"jetbrains.com.cn\", \"www.jetbrains.com.cn\",\n",
    "    }\n",
    "    results = serpapi_search(q, exclude_domains=merged_excludes, num=num)\n",
    "\n",
    "    urls = [r.get(\"url\", \"\") for r in results if r.get(\"url\")]\n",
    "    return urls[:num]\n",
    "\n",
    "# --- 1) Define the tool schema the model can call ---------------------------\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"serpapi_search_tool\",\n",
    "            \"description\": \"Run a Google search via SerpAPI and return third-party URLs (JetBrains excluded).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"q\": {\"type\": \"string\", \"description\": \"The exact Google query to run.\"},\n",
    "                    \"max\": {\"type\": \"integer\", \"description\": \"Max URLs to return (<=10).\", \"default\": 5},\n",
    "                    \"exclude\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Full URLs or domains to exclude\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"q\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 2) Prepare the conversation -------------------------------------------\n",
    "def source_intro_before_locating(url: str) -> str:\n",
    "    txt = _fetch_text(url)\n",
    "    m = re.search(r\"Locating\\s+this\\s+inspection\", txt, flags=re.IGNORECASE)\n",
    "    return txt[:m.start()].strip() if m else txt.strip()\n",
    "\n",
    "def build_context(rule: dict | str) -> str:\n",
    "    gpt4_desc = load_gpt4_rule_description()  # may be \"\"\n",
    "    rule_text = (rule.get(\"rule\") or rule.get(\"name\") or rule.get(\"description\")) if isinstance(rule, dict) else str(rule)\n",
    "    intro = source_intro_before_locating(SOURCE)\n",
    "    context = re.sub(r\"\\s+\", \" \", f\"{FRAMEWORK} {intro} {gpt4_desc} {rule_text} {TOPIC}\").strip()\n",
    "    return context\n",
    "\n",
    "system_msg = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        \"You are an analyst. Decide the BEST Google query for SerpAPI to find a NEW third-party page that \"\n",
    "        \"specifically addresses the rule context. Prefer queries that are precise (use allintext/intitle if helpful). \"\n",
    "        \"Do NOT return JetBrains links. When you need to search, CALL the serpapi_search_tool exactly once with your query.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def ask_model_to_search(rule, excluded: set):\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Context for the rule:\\n\"\n",
    "            f\"{build_context(rule)}\\n\\n\"\n",
    "            \"Goal: Find ONE new relevant third-party URL that discusses this rule/topic. \"\n",
    "            \"Return the single best URL and a one-sentence why-it’s-relevant.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # --- 3) First turn: model decides the query and issues a tool call ------\n",
    "    first = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=[system_msg, user_msg],\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "\n",
    "    msg = first.choices[0].message\n",
    "    tool_calls = getattr(msg, \"tool_calls\", None)\n",
    "\n",
    "    if not tool_calls:\n",
    "        return {\"url\": \"\", \"rationale\": msg.content or \"\"}\n",
    "\n",
    "    tc = tool_calls[0]\n",
    "    if tc.function.name != \"serpapi_search_tool\":\n",
    "        return {\"url\": \"\", \"rationale\": \"Model called unexpected tool.\"}\n",
    "\n",
    "    # Parse tool args\n",
    "    args = json.loads(tc.function.arguments or \"{}\")\n",
    "    q = args.get(\"q\", \"\")\n",
    "    k = int(args.get(\"max\", 5))\n",
    "    ex = set(args.get(\"exclude\") or set()) | excluded\n",
    "\n",
    "    # Run the tool (SerpAPI) ourselves\n",
    "    urls = serpapi_search_excluding(q, ex, num=min(k, 10))\n",
    "\n",
    "    # --- 4) Send tool results back to the model -----------------------------\n",
    "    tool_result_msg = {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tc.id,\n",
    "        \"name\": \"serpapi_search_tool\",\n",
    "        \"content\": json.dumps({\"query\": q, \"results\": urls}, ensure_ascii=False)\n",
    "    }\n",
    "\n",
    "    final = client.chat.completions.create(\n",
    "        model=\"gpt-5\",\n",
    "        messages=[system_msg, user_msg, msg, tool_result_msg]\n",
    "    )\n",
    "\n",
    "    out = final.choices[0].message.content.strip() if final.choices else \"\"\n",
    "    m = re.search(r\"(https?://\\S+)\", out)\n",
    "    best_url = m.group(1) if m else (urls[0] if urls else \"\")\n",
    "    return {\"url\": best_url, \"rationale\": out}\n",
    "\n",
    "# --- 5) Usage ---------------------------------------------------------------\n",
    "# Build the live exclusion set (exact URLs + domains)\n",
    "gpt5_found_urls = load_gpt5_found_urls()  # your persisted set\n",
    "current_exclusions = set(EXCLUDED_DOMAINS) | set(gpt5_found_urls)\n",
    "\n",
    "\n",
    "result = ask_model_to_search(rule={\"rule\": TOPIC}, excluded=current_exclusions)\n",
    "print(\"Chosen URL:\", result[\"url\"])\n",
    "print(\"Model rationale:\\n\", result[\"rationale\"])\n",
    "\n",
    "if result[\"url\"]:\n",
    "    save_gpt5_found_urls({result[\"url\"]})\n",
    "\n",
    "framework = FRAMEWORK\n",
    "topic = TOPIC\n",
    "jetbrains_intro = load_new_rule_explanation(GPT4_RULE_DESCRIPTION_PATH)\n",
    "gpt5_expl = load_gpt4_rule_description(newRULE_EXPLANATIONS_PATH)\n",
    "\n",
    "top = search_framework_find(framework, topic, jetbrains_intro, gpt5_expl)\n",
    "for r in top[:5]:\n",
    "    print(r[\"url\"], \"—\", r[\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adf5aed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:31.168974Z",
     "iopub.status.busy": "2025-11-13T03:43:31.168974Z",
     "iopub.status.idle": "2025-11-13T03:43:31.911470Z",
     "shell.execute_reply": "2025-11-13T03:43:31.911470Z"
    },
    "id": "rLMbveJ2YIt3",
    "outputId": "938cef1c-ab2a-481c-b065-0aefc0ac9d00",
    "papermill": {
     "duration": 0.758293,
     "end_time": "2025-11-13T03:43:31.911470",
     "exception": false,
     "start_time": "2025-11-13T03:43:31.153177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 SerpAPI query:\n",
      "allintext:(\"@PathVariable\" \"Spring MVC\" mismatch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ SerpAPI request failed.\n",
      "⚠️ No URLs found — query may be too specific or context too long. Try shortening the input.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# --- Rule Validation Utilities (Using SerpAPI) ---\n",
    "# --- Naive approach of combining all contexts in text form\n",
    "import html\n",
    "\n",
    "def _extract_domain_u(url: str) -> str:\n",
    "    try:\n",
    "        host = re.sub(r\"^https?://\", \"\", url, flags=re.IGNORECASE).split(\"/\")[0]\n",
    "        return host.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def is_excluded_u(url: str, excluded_domains: Set[str]) -> bool:\n",
    "    \"\"\"Return True if the URL's host matches any excluded domain.\"\"\"\n",
    "    if not excluded_domains:\n",
    "        return False\n",
    "    host = _extract_domain_u(url)\n",
    "    return any(host == dom or host.endswith(f\".{dom}\") for dom in excluded_domains)\n",
    "\n",
    "def serpapi_search_urls(query: str, exclude_domains: Optional[Set[str]] = None, num: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Searches Google via SerpAPI and returns URLs that are *not* excluded.\n",
    "    \"\"\"\n",
    "    if not SERPAPI_API_KEY:\n",
    "        print(\"⚠️ No SerpAPI key provided.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        params = {\"engine\": \"google\", \"q\": query, \"api_key\": SERPAPI_API_KEY, \"num\": str(num)}\n",
    "        r = requests.get(\"https://serpapi.com/search\", params=params, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        out: List[str] = []\n",
    "        for item in data.get(\"organic_results\", []):\n",
    "            url = item.get(\"link\")\n",
    "            if url and not is_excluded_u(url, EXCLUDED_DOMAINS):\n",
    "                out.append(url)\n",
    "        return out\n",
    "    except Exception:\n",
    "        print(\"⚠️ SerpAPI request failed.\")\n",
    "        return []\n",
    "\n",
    "# def build_query_from_rule(rule: dict | str) -> str:\n",
    "#     \"\"\"\n",
    "#     Build a SerpAPI query using:\n",
    "#       - The SOURCE intro (before 'Locating this inspection')\n",
    "#       - The saved GPT-4 rule description (if available)\n",
    "#       - FRAMEWORK and TOPIC context\n",
    "#     Adds Google's 'allintext:' operator to focus on pages that contain all contextual words.\n",
    "#     \"\"\"\n",
    "#     # 1️⃣ Load saved GPT-4 rule description (if any)\n",
    "#     _ = load_gpt4_rule_description()\n",
    "\n",
    "#     # 2️⃣ Get Inspectopedia intro (before 'Locating this inspection')\n",
    "#     intro = source_intro_before_locating(SOURCE)\n",
    "\n",
    "#     # 3️⃣ Get explanation of the new generated rule\n",
    "#     newR = load_gpt4_rule_description(newRULE_EXPLANATIONS_PATH)\n",
    "\n",
    "#     # 4️⃣ Combine minimal context\n",
    "#     combined_context = f\"{FRAMEWORK} {intro} {newR} {TOPIC}\"\n",
    "#     combined_context = re.sub(r\"\\s+\", \" \", combined_context).strip()\n",
    "#     print(\"Combined context:\", combined_context)\n",
    "\n",
    "#     # 5️⃣ Return a valid Google query\n",
    "#     return f'allintext:(\"{TOPIC}\" {combined_context})'\n",
    "\n",
    "\n",
    "def build_query_from_rule(rule: dict | str) -> str:\n",
    "\n",
    "    terms = ['\"@PathVariable\"', '\"Spring MVC\"', \"mismatch\"]\n",
    "\n",
    "    return f'allintext:({\" \".join(terms)})'\n",
    "\n",
    "\n",
    "\n",
    "def validate_rule_via_serpapi(rule_text: str,\n",
    "                              excluded_domains: Optional[Set[str]] = None,\n",
    "                              max_urls: int = 3) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Uses SerpAPI with an allintext-based query for higher relevancy.\n",
    "    Returns up to `max_urls` URLs excluding JetBrains and other specified domains.\n",
    "    \"\"\"\n",
    "    q = build_query_from_rule(rule_text)\n",
    "    print(f\"🔍 SerpAPI query:\\n{q}\\n\")\n",
    "\n",
    "    urls = serpapi_search_urls(q, excluded_domains or set())\n",
    "\n",
    "    if not urls:\n",
    "        print(\"⚠️ No URLs found — query may be too specific or context too long. Try shortening the input.\")\n",
    "    else:\n",
    "        print(f\"✅ Found {len(urls)} URLs:\")\n",
    "        for u in urls[:max_urls]:\n",
    "            print(\" •\", u)\n",
    "\n",
    "    return {\"query\": q, \"urls\": urls[:max_urls]}\n",
    "\n",
    "\n",
    "query = build_query_from_rule({\"content\": CONTENT})\n",
    "r = validate_rule_via_serpapi(query)\n",
    "urls_only = r[\"urls\"]\n",
    "print(urls_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bf06a7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:31.931045Z",
     "iopub.status.busy": "2025-11-13T03:43:31.931045Z",
     "iopub.status.idle": "2025-11-13T03:43:31.949585Z",
     "shell.execute_reply": "2025-11-13T03:43:31.949585Z"
    },
    "id": "WZ2TtQcKJuBo",
    "outputId": "f2413c5f-30c1-4887-e5e0-6e5a94532b08",
    "papermill": {
     "duration": 0.034576,
     "end_time": "2025-11-13T03:43:31.949585",
     "exception": false,
     "start_time": "2025-11-13T03:43:31.915009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No URLs found in SerpAPI output — nothing to append.\n",
      "                                 framework  \\\n",
      "0  Contexts and Dependency Injection (CDI)   \n",
      "\n",
      "                     source URL (JetBrains web page)  \\\n",
      "0  https://www.jetbrains.com.cn/en-us/help/inspec...   \n",
      "\n",
      "  brief_description of the content in the source URL  \\\n",
      "0  The JetBrains Inspectopedia page provides info...   \n",
      "\n",
      "                                GPT-4 generated_rule  \\\n",
      "0  Rule incorrect-bean-definitions {\\n  for (file...   \n",
      "\n",
      "               generated_rule_explanation from GPT-4  \\\n",
      "0  This rule, named 'incorrect-bean-definitions',...   \n",
      "\n",
      "   new_built-in_functions_explanation if any  \\\n",
      "0                                        NaN   \n",
      "\n",
      "                                PRE_model_validation  \\\n",
      "0  The rule is syntactically correct and follows ...   \n",
      "\n",
      "  3rd-party most relevant URLs and summary for each URL  \\\n",
      "0  1. https://www.baeldung.com/cdi-qualifiers: Th...      \n",
      "\n",
      "   POST_model validation SerpAPI-found URLs  \n",
      "0                    NaN                     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Appending SerpAPI-found-URL into the CSV file (local)\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NEW_URLS_COLUMN = \"SerpAPI-found URLs\"\n",
    "\n",
    "def append_serpapi_urls_to_csv(serpapi_output: dict,\n",
    "                               csv_path: str = CSV_PATH,\n",
    "                               column: str = NEW_URLS_COLUMN):\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"❌ CSV not found at {csv_path}. Please verify the path.\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if column not in df.columns:\n",
    "        df[column] = \"\"\n",
    "\n",
    "\n",
    "    new_urls = list(serpapi_output.get(\"urls\", []) or [])\n",
    "    if not new_urls:\n",
    "        print(\"⚠️ No URLs found in SerpAPI output — nothing to append.\")\n",
    "        return df\n",
    "\n",
    "    new_urls_str = \"; \".join(new_urls)\n",
    "\n",
    "    if len(df) > 0:\n",
    "        last_idx = df.index[-1]\n",
    "        existing = str(df.at[last_idx, column])\n",
    "        if existing and isinstance(existing, str):\n",
    "            combined = set(existing.split(\"; \")) | set(new_urls)\n",
    "            df.at[last_idx, column] = \"; \".join(sorted(combined))\n",
    "        else:\n",
    "            df.at[last_idx, column] = new_urls_str\n",
    "    else:\n",
    "        df = pd.DataFrame([{column: new_urls_str}])\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Appended {len(new_urls)} SerpAPI URL(s) to {csv_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "updated_df = append_serpapi_urls_to_csv(r)\n",
    "print(updated_df.tail(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdace399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:31.962627Z",
     "iopub.status.busy": "2025-11-13T03:43:31.962627Z",
     "iopub.status.idle": "2025-11-13T03:43:31.978754Z",
     "shell.execute_reply": "2025-11-13T03:43:31.978754Z"
    },
    "id": "M6Pnxs26WarE",
    "papermill": {
     "duration": 0.017649,
     "end_time": "2025-11-13T03:43:31.980276",
     "exception": false,
     "start_time": "2025-11-13T03:43:31.962627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### STILL NOT THE ANSWER WEB PAGE IS LOCATED!!! ###\n",
    "# ## I thought I need (1) better query construction, (2) multi-query searching, and (3) a simple reranker.\n",
    "# # Below is a drop-in that fixes bugs, adds strong Google operators, runs several candidates, and\n",
    "# # ranks results by “Hibernate/@Find”-likeness. I also add clean exclusions + host priorities.\n",
    "\n",
    "# from __future__ import annotations\n",
    "# import os, re, time, json, math\n",
    "# from typing import List, Optional, Set, Dict, Iterable, Tuple\n",
    "# from urllib.parse import urlparse\n",
    "# import requests\n",
    "\n",
    "# # -------------------------------\n",
    "# # Config\n",
    "# # -------------------------------\n",
    "# SERPAPI_API_KEY = userdata.get(\"SERPAPI_API_KEY\")\n",
    "# DEFAULT_EXCLUDES: Set[str] = {\n",
    "#     \"facebook.com\",\"x.com\",\"twitter.com\",\"pinterest.com\",\"youtube.com\",\n",
    "#     \"linkedin.com\",\"tiktok.com\",\"instagram.com\",\"buffercdn.com\"\n",
    "# }\n",
    "\n",
    "# # Prefer official docs that likely contain @Find\n",
    "# HOST_PRIORITY: Dict[str, int] = {\n",
    "#     \"docs.jboss.org\": 100,             # Hibernate javadocs\n",
    "#     \"hibernate.org\": 95,               # Hibernate website\n",
    "#     \"jakarta.ee\": 80,                  # EE docs\n",
    "#     \"jetbrains.com\": 70,               # Inspectopedia/IDEA docs\n",
    "#     \"github.com\": 40,                  # Code often useful\n",
    "# }\n",
    "\n",
    "# # -------------------------------\n",
    "# # Helpers\n",
    "# # -------------------------------\n",
    "# def host_of(url: str) -> str:\n",
    "#     try:\n",
    "#         return urlparse(url).netloc.lower()\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def is_excluded(url: str, excluded_hosts: Set[str]) -> bool:\n",
    "#     h = host_of(url)\n",
    "#     return any(h == ex or h.endswith(f\".{ex}\") for ex in excluded_hosts)\n",
    "\n",
    "# def uniq_preserve_order(seq: Iterable[str]) -> List[str]:\n",
    "#     seen = set()\n",
    "#     out = []\n",
    "#     for s in seq:\n",
    "#         if s not in seen:\n",
    "#             seen.add(s)\n",
    "#             out.append(s)\n",
    "#     return out\n",
    "\n",
    "# # -------------------------------\n",
    "# # SerpAPI search (multi-query)\n",
    "# # -------------------------------\n",
    "# def serpapi_search(query: str, exclude_domains: Optional[Set[str]] = None, num: int = 10) -> List[dict]:\n",
    "#     \"\"\"\n",
    "#     Returns a *list of result dicts* (we keep title/snippet for reranking).\n",
    "#     \"\"\"\n",
    "#     if not SERPAPI_API_KEY:\n",
    "#         print(\"⚠️ No SerpAPI key provided.\")\n",
    "#         return []\n",
    "\n",
    "#     excludes = set(DEFAULT_EXCLUDES)\n",
    "#     if exclude_domains:\n",
    "#         excludes |= set(exclude_domains)\n",
    "\n",
    "#     params = {\"engine\": \"google\", \"q\": query, \"api_key\": SERPAPI_API_KEY, \"num\": str(num)}\n",
    "#     try:\n",
    "#         r = requests.get(\"https://serpapi.com/search\", params=params, timeout=20)\n",
    "#         r.raise_for_status()\n",
    "#         data = r.json()\n",
    "#         out = []\n",
    "#         for item in data.get(\"organic_results\", []):\n",
    "#             url = item.get(\"link\")\n",
    "#             if not url:\n",
    "#                 continue\n",
    "#             if is_excluded(url, excludes):\n",
    "#                 continue\n",
    "#             out.append({\n",
    "#                 \"url\": url,\n",
    "#                 \"title\": item.get(\"title\", \"\"),\n",
    "#                 \"snippet\": item.get(\"snippet\", \"\"),\n",
    "#                 \"position\": item.get(\"position\"),\n",
    "#             })\n",
    "#         return out\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ SerpAPI request failed: {e}\")\n",
    "#         return []\n",
    "\n",
    "# # -------------------------------\n",
    "# # Query building\n",
    "# # -------------------------------\n",
    "# def normalize_space(s: str) -> str:\n",
    "#     return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "# def key_terms_from_text(text: str, k: int = 8) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Cheap, dependency-free keyterm picker: keep tokens with @, camelCase, or java-ish/Dot terms; fallback to frequent words.\n",
    "#     Replace with RAKE/TextRank if you like.\n",
    "#     \"\"\"\n",
    "#     if not text:\n",
    "#         return []\n",
    "#     toks = re.findall(r\"[@\\w\\.]+\", text)\n",
    "#     # keep distinctive tokens\n",
    "#     candidates = [t for t in toks if len(t) > 2]\n",
    "#     # light heuristic boosts\n",
    "#     def score(t: str) -> float:\n",
    "#         s = 0.0\n",
    "#         if t.startswith(\"@\"): s += 3\n",
    "#         if \".\" in t: s += 1.5   # package/class names\n",
    "#         if re.search(r\"[A-Z][a-z]+[A-Z]\", t): s += 1.0  # camelCase\n",
    "#         if t.lower() in {\"find\",\"finder\",\"annotation\",\"hibernate\",\"entity\",\"typedquery\",\"selectionquery\"}:\n",
    "#             s += 1.2\n",
    "#         return s\n",
    "#     ranked = sorted(candidates, key=lambda t: (score(t), len(t)), reverse=True)\n",
    "#     out = uniq_preserve_order(ranked)[:k]\n",
    "#     return out\n",
    "\n",
    "# def build_candidate_queries(framework: str, topic: str, jetbrains_intro: str, gpt5_expl: str) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Generate several strong Google queries with operators. We’ll try them all and then rerank results.\n",
    "#     \"\"\"\n",
    "#     ctx = normalize_space(\" \".join([framework or \"\", topic or \"\", jetbrains_intro or \"\", gpt5_expl or \"\"]))\n",
    "#     terms = key_terms_from_text(ctx)\n",
    "#     joined = \" \".join(terms) if terms else ctx\n",
    "\n",
    "#     # Pay special attention to '@Find' literal\n",
    "#     base = '\"@Find\" hibernate annotation'\n",
    "\n",
    "#     candidates = [\n",
    "#         # precision: official javadocs\n",
    "#         f'site:docs.jboss.org {base}',\n",
    "#         f'site:hibernate.org {base}',\n",
    "#         # title/url hints\n",
    "#         f'intitle:@Find hibernate annotation',\n",
    "#         f'inurl:hibernate intitle:@Find annotation',\n",
    "#         # allintext to bind concepts\n",
    "#         f'allintext:@Find hibernate entity finder {joined}',\n",
    "#         # combine with JetBrains if trying to verify Inspectopedia rule\n",
    "#         f'site:jetbrains.com Inspectopedia \"@Find\" hibernate',\n",
    "#         # fallback broad\n",
    "#         f'{base} {joined}',\n",
    "#     ]\n",
    "\n",
    "#     # de-duplicate + keep short, valid strings\n",
    "#     return uniq_preserve_order([normalize_space(c) for c in candidates if c.strip()])\n",
    "\n",
    "# # -------------------------------\n",
    "# # Lightweight reranking\n",
    "# # -------------------------------\n",
    "# def feature_score(result: dict) -> float:\n",
    "#     \"\"\"\n",
    "#     Score by: host priority, presence of '@Find' in title/snippet/url,\n",
    "#     path hints like '/annotations/processing/Find'.\n",
    "#     \"\"\"\n",
    "#     url, title, snip = result.get(\"url\",\"\"), result.get(\"title\",\"\"), result.get(\"snippet\",\"\")\n",
    "#     host = host_of(url)\n",
    "#     score = HOST_PRIORITY.get(host, 0)\n",
    "\n",
    "#     text = \" \".join([url, title, snip]).lower()\n",
    "#     boosts = [\n",
    "#         (r\"@find\", 15),\n",
    "#         (r\"\\bhibernate\\b\", 8),\n",
    "#         (r\"\\bannotation(s)?\\b\", 5),\n",
    "#         (r\"/annotations/processing/find\", 12),\n",
    "#         (r\"/javadocs/\", 10),\n",
    "#         (r\"\\bentity\\b\", 4),\n",
    "#     ]\n",
    "#     for pat, w in boosts:\n",
    "#         if re.search(pat, text):\n",
    "#             score += w\n",
    "\n",
    "#     # slight boost for earlier rank\n",
    "#     pos = result.get(\"position\")\n",
    "#     if isinstance(pos, int):\n",
    "#         score += max(0, 10 - pos)\n",
    "\n",
    "#     return float(score)\n",
    "\n",
    "# def rerank(results: List[dict]) -> List[dict]:\n",
    "#     return sorted(results, key=feature_score, reverse=True)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Orchestrator\n",
    "# # -------------------------------\n",
    "# def search_framework_find(framework: str, topic: str,\n",
    "#                           jetbrains_intro: str, gpt5_expl: str,\n",
    "#                           exclude_domains: Optional[Set[str]] = None,\n",
    "#                           per_query: int = 10) -> List[dict]:\n",
    "#     queries = build_candidate_queries(framework, topic, jetbrains_intro, gpt5_expl)\n",
    "\n",
    "#     all_results: List[dict] = []\n",
    "#     for q in queries:\n",
    "#         batch = serpapi_search(q, exclude_domains=exclude_domains, num=per_query)\n",
    "#         all_results.extend(batch)\n",
    "#         time.sleep(0.2)  # be polite; SerpAPI handles rate limiting but avoid bursts\n",
    "\n",
    "#     # Deduplicate by URL\n",
    "#     dedup: Dict[str, dict] = {}\n",
    "#     for r in all_results:\n",
    "#         dedup[r[\"url\"]] = r\n",
    "#     ranked = rerank(list(dedup.values()))\n",
    "#     return ranked[:20]  # top-N\n",
    "\n",
    "\n",
    "# framework = FRAMEWORK            # e.g., \"Hibernate ORM 6.x\"\n",
    "# topic = TOPIC                    # e.g., \"Finder methods with @Find\"\n",
    "# jetbrains_intro = load_new_rule_explanation(GPT5_RULE_DESCRIPTION_PATH)   # scraped from JetBrains web page\n",
    "\n",
    "# gpt5_expl = load_gpt5_rule_description(newRULE_EXPLANATIONS_PATH)              # your own summary, optional\n",
    "\n",
    "# top = search_framework_find(framework, topic, jetbrains_intro, gpt5_expl)\n",
    "# for r in top[:5]:\n",
    "#     print(r[\"url\"], \"—\", r[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd4517",
   "metadata": {
    "id": "20MQAaIyYx45",
    "papermill": {
     "duration": 0.014231,
     "end_time": "2025-11-13T03:43:31.994507",
     "exception": false,
     "start_time": "2025-11-13T03:43:31.980276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Yes, the answer web page was \"https://docs.jboss.org/hibernate/orm/7.0/javadocs/org/hibernate/annotations/processing/Find.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "269972d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:32.010317Z",
     "iopub.status.busy": "2025-11-13T03:43:32.010317Z",
     "iopub.status.idle": "2025-11-13T03:43:32.017701Z",
     "shell.execute_reply": "2025-11-13T03:43:32.017701Z"
    },
    "id": "ct5J7m_YMlec",
    "papermill": {
     "duration": 0.023194,
     "end_time": "2025-11-13T03:43:32.017701",
     "exception": false,
     "start_time": "2025-11-13T03:43:31.994507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### TO-DOs ######\n",
    "# What if the one generated rule is too big?\n",
    "# Save the generated rule in json file or text file first then consult with Codex-GPT-5\n",
    "# To optimize the propmt, look into DsPY: https://adasci.org/dspy-streamlining-llm-prompt-optimization/\n",
    "# Using DsPy, we’ll configure our Language Model (GPT-3.5-turbo) and Retrieval Model (ColBERTv2). These will form the backbone of our RAG system.\n",
    "# Codex-GPT-5 could help break down the one (big) generated rule if it exceeds N lines - - will this be meaninful in terms of validation and running the MeCheck engine to check more granular rules?\n",
    "# Then, iterate the validation process with each sub-rule derived from the original rule both with GPT-5 and SerpAPI\n",
    "# We will Hibernate's incorrect @find annotation rule will be implemented into the current MeCheck engine\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e1bd094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:32.026426Z",
     "iopub.status.busy": "2025-11-13T03:43:32.026426Z",
     "iopub.status.idle": "2025-11-13T03:43:32.044466Z",
     "shell.execute_reply": "2025-11-13T03:43:32.044466Z"
    },
    "id": "_N7Jy1pWO88F",
    "papermill": {
     "duration": 0.01804,
     "end_time": "2025-11-13T03:43:32.044466",
     "exception": false,
     "start_time": "2025-11-13T03:43:32.026426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################################OLD or MERGED FUNCTIONS ##############\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "# To save the gpt5-generated rule description based on the scraped content from the JetBrains webpage\n",
    "# import os, re, json, requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# GPT5_RULE_DESCRIPTION_PATH = \"/content/gpt5_rule_description_scraped_content.json\"\n",
    "\n",
    "\n",
    "# # --- Save column “generated_rule_explanation from GPT-5” to JSON (list of strings) ---\n",
    "\n",
    "# generated_explanations = [str(x).strip() for x in df[\"generated_rule_explanation from GPT-5\"].fillna(\"\") if str(x).strip()]\n",
    "# single_explanation = generated_explanations[0] if generated_explanations else \"\"\n",
    "# with open(newRULE_EXPLANATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(generated_explanations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(os.path.dirname(newRULE_EXPLANATIONS_PATH), exist_ok=True)\n",
    "#     with open(newRULE_EXPLANATIONS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(\n",
    "#             {\"new_generated_rule_explanation\": single_explanation},\n",
    "#             f,\n",
    "#             ensure_ascii=False,\n",
    "#             indent=2\n",
    "#         )\n",
    "#     print(f\"✅ Saved new_generated_rule_explanation to {newRULE_EXPLANATIONS_PATH}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Failed to save JSON: {e}\")\n",
    "\n",
    "\n",
    "# def fetch_text(url: str, timeout: int = 20) -> str:\n",
    "#     \"\"\"\n",
    "#     Fetches text content from the SOURCE URL.\n",
    "#     Cleans out scripts, styles, and noscript tags.\n",
    "#     \"\"\"\n",
    "#     url = SOURCE  # Always use the global SOURCE variable\n",
    "#     resp = requests.get(url, timeout=timeout)\n",
    "#     resp.raise_for_status()\n",
    "#     soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#     for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "#         tag.decompose()\n",
    "#     text = soup.get_text(separator=\"\\n\")\n",
    "#     text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "# def source_intro_saved_json(url: str, path: str = GPT5_RULE_DESCRIPTION_PATH) -> str:\n",
    "#     \"\"\"\n",
    "#     Scrapes and saves only the portion of the JetBrains Inspectopedia page\n",
    "#     before the 'Locating this inspection' section into a JSON file.\n",
    "\n",
    "#     The text is also returned so you can preview or use it immediately.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         full = fetch_text(url)\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to fetch text from {url}: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "#     # Find the section before \"Locating this inspection\"\n",
    "#     m = re.search(r\"^\\s*Locating\\s+this\\s+inspection\\b\", full, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#     if m:\n",
    "#         content_before = full[:m.start()].strip()\n",
    "#     else:\n",
    "#         content_before = full.strip()\n",
    "\n",
    "#     # Save to JSON file\n",
    "#     try:\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#             json.dump(\n",
    "#                 {\"jetbrains_scraped_rule_description\": content_before},\n",
    "#                 f,\n",
    "#                 ensure_ascii=False,\n",
    "#                 indent=2\n",
    "#             )\n",
    "#         print(f\"✅ Saved JetBrains scraped rule description context to {path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to save JSON: {e}\")\n",
    "\n",
    "#     return content_before\n",
    "\n",
    "\n",
    "# def load_new_rule_explanation(path: str = GPT5_RULE_DESCRIPTION_PATH) -> str:\n",
    "#     \"\"\"\n",
    "#     Loads the previously saved GPT-5 rule description for contextual use in SerpAPI searches.\n",
    "#     Returns an empty string if none exists or file is corrupted.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(path):\n",
    "#         print(\"⚠️ No saved GPT-5 rule description found.\")\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             data = json.load(f)\n",
    "#         return data.get(\"new_generated_rule_explanation\", \"\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to load saved description: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# def load_gpt5_rule_description(path: str = newRULE_EXPLANATIONS_PATH) -> str:\n",
    "#     \"\"\"\n",
    "#     Loads the new-GPT-5-generated rule explanation to provide context to SerpAPI later.\n",
    "#     Returns an empty string if none exists or file is corrupted.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(path):\n",
    "#         print(\"⚠️ No new-GPT-5 rule explanation found.\")\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             data = json.load(f)\n",
    "#         return data.get(\"generated_rule_explanation from GPT-5\", \"\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Failed to load saved description: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "# # Step 1: Scrape and save\n",
    "# context_text = source_intro_saved_json(SOURCE)\n",
    "# print(\"\\n--- Preview of saved content ---\\n\")\n",
    "# print(context_text[:500], \"...\")  # show the first 500 chars\n",
    "\n",
    "# # Step 2: Load later (e.g., in a new session)\n",
    "# loaded_text = load_new_rule_explanation(GPT5_RULE_DESCRIPTION_PATH)\n",
    "# print(\"\\n--- Loaded back from JSON scraped from JetBrains  ---\\n\")\n",
    "# print(loaded_text[:500], \"...\")\n",
    "\n",
    "# # Step 3: Load later (e.g., in a new session)\n",
    "# loaded_text1 = load_gpt5_rule_description(newRULE_EXPLANATIONS_PATH)\n",
    "# print(\"\\n--- Loaded back from JSON from gpt5_rule_explanation ---\\n\")\n",
    "# print(loaded_text1[:500], \"...\")\n",
    "\n",
    "#source_intro_before_locating(SOURCE)\n",
    "\n",
    "\n",
    "# def fetch_text(url: str, timeout: int = 20) -> str:\n",
    "#     url = SOURCE\n",
    "#     resp = requests.get(url, timeout=timeout)\n",
    "#     resp.raise_for_status()\n",
    "#     soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#     for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "#         tag.decompose()\n",
    "#     text = soup.get_text(separator=\"\\n\")\n",
    "#     text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "#     return text\n",
    "\n",
    "# def source_intro_before_locating(url: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Returns ONLY the part of the page before the 'Locating this inspection' section.\n",
    "#     Matches the header case-insensitively and robustly.\n",
    "#     \"\"\"\n",
    "#     full = fetch_text(url)\n",
    "#     # Some pages include non-breaking / special spaces; keep the match flexible.\n",
    "#     m = re.search(r\"^\\s*Locating\\s+this\\s+inspection\\b\", full, flags=re.IGNORECASE | re.MULTILINE)\n",
    "#     if m:\n",
    "#         return full[:m.start()].strip()\n",
    "#         #print(full.strip())\n",
    "\n",
    "#         with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#              json.dump(full.strip(), f, ensure_ascii=False, indent=2)\n",
    "#              print(f\"✅ Saved GPT-5 rule description context to {path}\")\n",
    "#     return(full.strip())\n",
    "\n",
    "\n",
    "\n",
    "# def load_gpt5_rule_description(path: str = GPT5_RULE_DESCRIPTION_PATH) -> str:\n",
    "#     \"\"\"\n",
    "#     Loads the previously saved GPT-5 rule description for contextual use in SerpAPI searches.\n",
    "#     Returns an empty string if none exists.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(path):\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#             data = json.load(f)\n",
    "#         return data.get(\"gpt5_generated_rule_description\", \"\")\n",
    "#     except Exception:\n",
    "#         return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d34d684b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:32.058189Z",
     "iopub.status.busy": "2025-11-13T03:43:32.058189Z",
     "iopub.status.idle": "2025-11-13T03:43:32.070613Z",
     "shell.execute_reply": "2025-11-13T03:43:32.070613Z"
    },
    "id": "U8O00_TN7xTt",
    "papermill": {
     "duration": 0.026147,
     "end_time": "2025-11-13T03:43:32.070613",
     "exception": false,
     "start_time": "2025-11-13T03:43:32.044466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# #### OLD CODES ######### DO NOT RUN!!!\n",
    "# ########################################\n",
    "# # --- Rule Validation Utilities (Using SerpAPI) ---\n",
    "# import requests, time, html, re\n",
    "# from typing import List, Dict, Set, Optional\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# SERPAPI_API_KEY = userdata.get(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# DEFAULT_EXCLUDES = {\n",
    "#     \"jetbrains.com\",\n",
    "#     \"www.jetbrains.com\",\n",
    "#     \"jetbrains.com.cn\",\n",
    "#     \"www.jetbrains.com.cn\",\n",
    "# }\n",
    "\n",
    "# def is_excluded(url: str, exclude_domains: Set[str]) -> bool:\n",
    "#     if not exclude_domains:\n",
    "#         return False\n",
    "#     u = url.lower()\n",
    "#     return any(dom.lower() in u for dom in exclude_domains)\n",
    "\n",
    "# def serpapi_search(query: str, exclude_domains: Set[str] = None) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Search with SerpAPI and return non-excluded URLs.\n",
    "#     JetBrains domains are ALWAYS excluded to ensure third-party sources.\n",
    "#     \"\"\"\n",
    "#     if not SERPAPI_API_KEY:\n",
    "#         return []\n",
    "#     try:\n",
    "#         # merge user excludes with default JetBrains excludes\n",
    "#         merged_excludes = set(DEFAULT_EXCLUDES)\n",
    "#         if exclude_domains:\n",
    "#             merged_excludes |= set(exclude_domains)\n",
    "\n",
    "#         params = {\"engine\": \"google\",\"q\": query,\"api_key\": SERPAPI_API_KEY,\"num\": \"10\"}\n",
    "#         resp = requests.get(\"https://serpapi.com/search\", params=params, timeout=20)\n",
    "#         data = resp.json()\n",
    "#         results = []\n",
    "#         for item in data.get(\"organic_results\", []):\n",
    "#             url = item.get(\"link\")\n",
    "#             if url and not is_excluded(url, merged_excludes):\n",
    "#                 results.append(url)\n",
    "#         return results\n",
    "#     except Exception:\n",
    "#         return []\n",
    "\n",
    "# # def build_query_from_rule(rule_text: str) -> str:\n",
    "# #     tokens = [t for t in re.split(r'\\W+', rule_text) if t]\n",
    "# #     key = \" \".join(tokens[:8])\n",
    "# #     return f\"{key} error inspection rule\"\n",
    "\n",
    "# # def build_query_from_rule(rule_text: str) -> str:\n",
    "# #     m = re.search(r'Rule\\s+([\\w\\-]+)', rule_text)\n",
    "# #     print(\"Rule name:\", m.group(1) if m else \"unavailable\")\n",
    "# #     rule_name = m.group(1) if m else f\"{TOPIC}\"\n",
    "# #     return f\"{rule_name} {TOPIC}\"\n",
    "# def build_query_from_rule(rule: dict | str) -> str:\n",
    "#     \"\"\"\n",
    "#     Build a web search query using contextual information derived from:\n",
    "#       - The JetBrains Inspectopedia SOURCE page (up to 'Locating this inspection')\n",
    "#       - Its summarized content via GPT-5\n",
    "#       - The given rule (dict or string)\n",
    "\n",
    "#     The resulting query is semantically focused on the rule’s core description,\n",
    "#     not its literal name, and ends with FRAMEWORK and TOPIC.\n",
    "#     \"\"\"\n",
    "#     # 1️⃣ Extract rule text (fallback to string)\n",
    "#     if isinstance(rule, dict):\n",
    "#         base_text = rule.get(\"rule\") or rule.get(\"name\") or rule.get(\"description\") or \"\"\n",
    "#         print(\"base_text: \", base_text)\n",
    "#     else:\n",
    "#         base_text = str(rule)\n",
    "\n",
    "#     # 2️⃣ Fetch JetBrains Inspectopedia page content\n",
    "#     try:\n",
    "#         full_text = _fetch_text(SOURCE)\n",
    "#         # Keep only the portion before \"Locating this inspection\"\n",
    "#         cutoff = re.search(r\"Locating\\s+this\\s+inspection\", full_text, flags=re.IGNORECASE)\n",
    "#         if cutoff:\n",
    "#             source_text = full_text[:cutoff.start()]\n",
    "#         else:\n",
    "#             source_text = full_text\n",
    "#         source_text = source_text.strip()[:20000]  # safety cap\n",
    "#     except Exception as e:\n",
    "#         source_text = f\"(JetBrains fetch error: {e})\"\n",
    "\n",
    "#     # 3️⃣ Summarize JetBrains content (no temperature param)\n",
    "#     try:\n",
    "#         prompt = (\n",
    "#             \"Summarize the following JetBrains Inspectopedia documentation \"\n",
    "#             \"in 1-2 sentences focusing on the core inspection purpose or behavior:\\n\\n\"\n",
    "#             f\"{source_text}\"\n",
    "#         )\n",
    "#         comp = client.chat.completions.create(\n",
    "#             model=\"gpt-5\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         )\n",
    "#         jb_summary = comp.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         jb_summary = f\"Summary unavailable (GPT error: {e})\"\n",
    "\n",
    "#     # 4️⃣ Merge rule text + JetBrains summary → keywords\n",
    "#     tokens = [t for t in re.split(r'\\W+', f\"{jb_summary} {base_text}\") if t]\n",
    "#     key = \" \".join(tokens[:10]) if tokens else FRAMEWORK\n",
    "\n",
    "#     # 5️⃣ Return final query anchored to FRAMEWORK and TOPIC\n",
    "#     return f\"{key} {FRAMEWORK} {TOPIC}\"\n",
    "\n",
    "\n",
    "\n",
    "# def validate_rule_via_serpapi(rule_text: str,\n",
    "#                               exclude_domains: Optional[Set[str]] = None,\n",
    "#                               max_urls: int = 2) -> Dict[str, object]:\n",
    "#     q = build_query_from_rule(rule_text)\n",
    "#     urls = serpapi_search(q, exclude_domains or set())\n",
    "#     return {\"query\": q, \"urls\": urls[:max_urls]}\n",
    "\n",
    "# # JETBRAINS_DOC_URL = \"https://www.jetbrains.com.cn/en-us/help/inspectopedia/MnUnresolvedPathVariable.html\"\n",
    "\n",
    "# def _fetch_text(url: str, timeout: int = 20) -> str:\n",
    "#     resp = requests.get(url, timeout=timeout)\n",
    "#     resp.raise_for_status()\n",
    "#     soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#     for tag in soup([\"script\",\"style\",\"noscript\"]):\n",
    "#         tag.decompose()\n",
    "#     text = soup.get_text(separator=\"\\n\")\n",
    "#     text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text).strip()\n",
    "#     return text\n",
    "\n",
    "# def summarize_third_party(url: str, client, model: str = \"gpt-5\", temperature: float = 1) -> str:\n",
    "#     \"\"\"\n",
    "#     Fetch a third-party URL and summarize its content briefly.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         content = _fetch_text(url)[:20000]\n",
    "#     except Exception as e:\n",
    "#         return f\"Summary unavailable (fetch error: {e})\"\n",
    "#     prompt = (\n",
    "#         \"Summarize the following web page in 2-3 concise sentences, \"\n",
    "#         \"focusing on information relevant to static analysis rules or inspections.\\n\\n\"\n",
    "#         f\"Content:\\n{content}\"\n",
    "#     )\n",
    "#     try:\n",
    "#         comp = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "#             temperature=temperature,\n",
    "#         )\n",
    "#         return comp.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Summary unavailable (GPT error: {e})\"\n",
    "\n",
    "# def secondary_validation_via_jetbrains_gpt(rule_text: str,\n",
    "#                                            client,\n",
    "#                                            model: str = \"gpt-5\"\n",
    "#                                            ) -> str:\n",
    "#     \"\"\"\n",
    "#     Uses the JetBrains Inspectopedia page to validate the rule in one sentence.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         source_text = _fetch_text(SOURCE)[:30000]\n",
    "#     except Exception as e:\n",
    "#         return f\"Secondary validation skipped (JetBrains fetch error: {e})\"\n",
    "\n",
    "#     system_msg = (\n",
    "#         \"You are a precise static-analysis assistant. Validate a proposed rule \"\n",
    "#         \"STRICTLY using the provided JetBrains Inspectopedia excerpt. \"\n",
    "#         \"Respond with a single short sentence stating whether the rule aligns with the doc and why.\"\n",
    "#     )\n",
    "#     user_msg = (\n",
    "#         f\"Rule to validate:\\n{rule_text}\\n\\n\"\n",
    "#         f\"JetBrains Inspectopedia source (excerpt):\\n{source_text}\"\n",
    "#     )\n",
    "#     try:\n",
    "#         comp = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_msg},\n",
    "#                 {\"role\": \"user\", \"content\": user_msg},\n",
    "#             ],\n",
    "#             #temperature=temperature,\n",
    "#         )\n",
    "#         return comp.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Secondary validation failed to run GPT: {e}\"\n",
    "\n",
    "# def combined_secondary_validation(rule_text: str,\n",
    "#                                   third_party_text: str,\n",
    "#                                   client,\n",
    "#                                   model: str = \"gpt-5\"\n",
    "#                                  ) -> str:\n",
    "#     \"\"\"\n",
    "#     Validate the rule using BOTH JetBrains Inspectopedia (fetched live) and the third-party page text.\n",
    "#     Output: one short sentence that references alignment or mismatch.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         jb_text = _fetch_text(SOURCE)[:20000]\n",
    "#     except Exception as e:\n",
    "#         jb_text = f\"(JetBrains fetch error: {e})\"\n",
    "#     system_msg = (\n",
    "#         \"You are a precise static-analysis assistant. Validate the proposed rule \"\n",
    "#         \"STRICTLY using the provided JetBrains Inspectopedia excerpt AND the provided third-party page. \"\n",
    "#         \"Respond with one short sentence stating whether the rule is consistent with these sources and why.\"\n",
    "#     )\n",
    "#     user_msg = (\n",
    "#         f\"Rule:\\n{rule_text}\\n\\n\"\n",
    "#         f\"JetBrains Inspectopedia excerpt:\\n{jb_text}\\n\\n\"\n",
    "#         f\"Third-party page excerpt:\\n{third_party_text[:8000]}\"\n",
    "#     )\n",
    "#     try:\n",
    "#         comp = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_msg},\n",
    "#                 {\"role\": \"user\", \"content\": user_msg},\n",
    "#             ],\n",
    "#            # temperature=temperature,\n",
    "#         )\n",
    "#         return comp.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Combined validation failed (GPT error: {e})\"\n",
    "\n",
    "# print(\"✅ Helpers updated: SerpAPI now excludes JetBrains; added summarization and combined validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219e9374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:32.089828Z",
     "iopub.status.busy": "2025-11-13T03:43:32.089828Z",
     "iopub.status.idle": "2025-11-13T03:43:32.093178Z",
     "shell.execute_reply": "2025-11-13T03:43:32.093178Z"
    },
    "id": "LNsi59WG7xTu",
    "papermill": {
     "duration": 0.019028,
     "end_time": "2025-11-13T03:43:32.093178",
     "exception": false,
     "start_time": "2025-11-13T03:43:32.074150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# #### OLD CODES ######### DO NOT RUN\n",
    "# ########################################\n",
    "\n",
    "# # --- Demo: validate first generated rule (if available) ---\n",
    "# try:\n",
    "#     sample_rule = rules[0] if isinstance(rules, list) else rules\n",
    "#     if isinstance(sample_rule, dict) and \"rule\" in sample_rule:\n",
    "#         text = sample_rule[\"rule\"]\n",
    "#     else:\n",
    "#         text = json.dumps(sample_rule) if sample_rule else \"unavailable\"\n",
    "#     print(\"Rule sample:\", str(text)[:200], \"...\")\n",
    "#     print(\"\\n[Primary] SerpAPI URLs:\")\n",
    "#     out = validate_rule_via_serpapi(str(text), exclude_domains={\"jetbrains.com\",\"jetbrains.com.cn\",\"microsoft.com\"})\n",
    "#     for u in out[\"urls\"]:\n",
    "#         print(\" -\", u)\n",
    "#     print(\"\\n[Secondary] GPT-5 x JetBrains Inspectopedia:\")\n",
    "#     print(secondary_validation_via_jetbrains_gpt(str(text), client))\n",
    "# except Exception as e:\n",
    "#     print(\"Demo skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747feeda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T03:43:32.105724Z",
     "iopub.status.busy": "2025-11-13T03:43:32.105724Z",
     "iopub.status.idle": "2025-11-13T03:43:32.118299Z",
     "shell.execute_reply": "2025-11-13T03:43:32.118299Z"
    },
    "id": "v9j2bTsc7xTu",
    "papermill": {
     "duration": 0.025121,
     "end_time": "2025-11-13T03:43:32.118299",
     "exception": false,
     "start_time": "2025-11-13T03:43:32.093178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###########################################\n",
    "# #### OLD CODES ######### DO NOT RUN\n",
    "# ########################################\n",
    "# # --- Post-processing: save third-party URL, its summary, and GPT validation into CSV ---\n",
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# def _get_rule_text(x):\n",
    "#     if isinstance(x, dict):\n",
    "#         # try typical keys\n",
    "#         for k in (\"rule\", \"name\", \"title\", \"description\"):\n",
    "#             if k in x:\n",
    "#                 return str(x[k])\n",
    "#         return json.dumps(x)\n",
    "#     return str(x)\n",
    "\n",
    "# # Decide input set\n",
    "# try:\n",
    "#     df = pd.read_csv(\"rsl_generated_rules.csv\")\n",
    "#     # If rules variable exists, prefer it (might be fresher) and rebuild df\n",
    "#     if \"rules\" in globals():\n",
    "#         candidate = rules\n",
    "#         if isinstance(candidate, list):\n",
    "#             # try to preserve columns if dicts\n",
    "#             df = pd.DataFrame(candidate)\n",
    "#         else:\n",
    "#             df = pd.DataFrame([candidate])\n",
    "# except FileNotFoundError:\n",
    "#     # fall back to rules in-memory\n",
    "#     if \"rules\" in globals():\n",
    "#         candidate = rules\n",
    "#         if isinstance(candidate, list):\n",
    "#             df = pd.DataFrame(candidate)\n",
    "#         else:\n",
    "#             df = pd.DataFrame([candidate])\n",
    "#     else:\n",
    "#         raise RuntimeError(\"No rules found. Generate rules before running this cell.\")\n",
    "\n",
    "# if df.empty:\n",
    "#     raise RuntimeError(\"Rules dataframe is empty.\")\n",
    "\n",
    "# third_urls = []\n",
    "# third_summaries = []\n",
    "# gpt_validations = []\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     rule_text = _get_rule_text(row.to_dict())\n",
    "#     out = validate_rule_via_serpapi(rule_text, exclude_domains=set())  # JetBrains excluded by default\n",
    "#     url = out[\"urls\"][0] if out[\"urls\"] else \"\"\n",
    "#     third_urls.append(url)\n",
    "\n",
    "#     if url:\n",
    "#         try:\n",
    "#             page_text = _fetch_text(url)[:20000]\n",
    "#         except Exception as e:\n",
    "#             page_text = f\"(fetch error: {e})\"\n",
    "#         # summarize\n",
    "#         summ = summarize_third_party(url, client)\n",
    "#         third_summaries.append(summ)\n",
    "#         # combined validation (JetBrains + third-party content)\n",
    "#         valid = combined_secondary_validation(rule_text, page_text, client)\n",
    "#         gpt_validations.append(valid)\n",
    "#     else:\n",
    "#         third_summaries.append(\"No third-party page found\")\n",
    "#         gpt_validations.append(\"No third-party page; combined validation skipped\")\n",
    "\n",
    "# # Add/update columns\n",
    "# df[\"third_party_url\"] = third_urls\n",
    "# df[\"third_party_summary\"] = third_summaries\n",
    "# df[\"gpt_combined_validation\"] = gpt_validations\n",
    "\n",
    "# # Save back\n",
    "# df.to_csv(\"rsl_generated_rules.csv\", index=False)\n",
    "# print(\"✅ Updated rsl_generated_rules.csv with third_party_url, third_party_summary, gpt_combined_validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f688908",
   "metadata": {
    "id": "5GXoOXgJHkwb",
    "papermill": {
     "duration": 0.015765,
     "end_time": "2025-11-13T03:43:32.137639",
     "exception": false,
     "start_time": "2025-11-13T03:43:32.121874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 164.58501,
   "end_time": "2025-11-13T03:43:32.599114",
   "environment_variables": {},
   "exception": null,
   "input_path": "template.ipynb",
   "output_path": "runs\\121-Contexts_and_Dependency_Injection_CDI-Incorrect_bean_definitions_in_beans.xml.ipynb",
   "parameters": {
    "ARTIFACTS_DIR": "C:\\Users\\spenc\\Downloads\\local\\artifacts\\Incorrect_bean_definitions_in_beans.xml",
    "FRAMEWORK": "Contexts and Dependency Injection (CDI)",
    "OUT_DIR": "C:\\Users\\spenc\\Downloads\\local\\runs",
    "SOURCE": "https://www.jetbrains.com.cn/en-us/help/inspectopedia/CdiDomBeans.html",
    "TOPIC": "Incorrect bean definitions in beans.xml﻿"
   },
   "start_time": "2025-11-13T03:40:48.014104",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}